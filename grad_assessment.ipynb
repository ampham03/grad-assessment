{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "collapsed": true,
        "id": "yZWgNlEHOdQ3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34729cf6-21a2-4aaa-ddbd-59de26a89480"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8elshXOSl0cz",
        "outputId": "76e41939-42af-4dbc-e804-b32775b06077"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "BcGN5ejymSEV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "from datasets import load_dataset\n",
        "from nltk.translate.bleu_score import sentence_bleu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "collapsed": true,
        "id": "11RDYzzXTXOw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "f9c4f62fa256403983cc4f95cf82a3a6",
            "df919452c981461ebb634224e546c675",
            "dcd489fbc33b4163a6e54bf4384ccc43",
            "181077a5596f4460b56cf93d81fda50a",
            "60cf3cac9329420f93659b3291b6f3f8",
            "9122e39f324446d9aea0908f017438e3",
            "2a1829207ac3487b882fe4b94a973df2",
            "4b8fae2183cb44878f2baa7599801e00",
            "6c061e7036a7453cb9274db8059baa5f",
            "844a430e8f1a4e2fbc775f67ea1d10ab",
            "140c3067e3554c32ae6b29d00392d175",
            "2d527acf3231437a94541cd059afb91b",
            "b314b67e1c2641a19a5d1d7deeb2ba5b",
            "9a01ddff9be9495cba21abbe2321fa32",
            "1a952edcc18c494183bc55de334d58f6",
            "67b56fb927a04154a958d2bd29ad425a",
            "3764b376b2ca4e25be1f12cf391e8694",
            "0e8a01f09ce84f3093ee064c8f84c7f8",
            "976e1192b88241b2826b068ef1616895",
            "f9d42e468e3c4898975f7a79b0030e81",
            "d2c625c6eee5443a9ef4ad953d5837f8",
            "466b3a9e70f6407c96c0c4db9e249648"
          ]
        },
        "outputId": "21224a5f-e1da-48a3-ac94-4663fe3b48df"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Resolving data files:   0%|          | 0/17 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f9c4f62fa256403983cc4f95cf82a3a6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Resolving data files:   0%|          | 0/17 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2d527acf3231437a94541cd059afb91b"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "model_name = \"Helsinki-NLP/opus-mt-en-de\"  # English to German\n",
        "model = MarianMTModel.from_pretrained(model_name)\n",
        "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# load WMT dataset for de-en\n",
        "de_en_dev = load_dataset(\"wmt18\", \"de-en\", split='validation')\n",
        "de_en_test = load_dataset(\"wmt18\", \"de-en\", split='test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0dnv3zCkbHm1"
      },
      "outputs": [],
      "source": [
        "# tokenize a sentence\n",
        "def tokenize(sentence):\n",
        "  return tokenizer(sentence, return_tensors=\"pt\", add_special_tokens=True).input_ids"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# decode a sequence of tokens\n",
        "def decode(token_ids):\n",
        "  return tokenizer.decode(token_ids, skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "uI6gGiqxFD5p"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "unTXIiDjvhTb"
      },
      "outputs": [],
      "source": [
        "class Hypothesis:\n",
        "  def __init__(self, score=0, is_open=True, sequence=torch.tensor([[model.config.decoder_start_token_id]]), constraints=[]):\n",
        "    self.score = score # keeps track of current sequence score\n",
        "    self.is_open = is_open # keeps track if the hypothesis is currently open\n",
        "    self.sequence = sequence # current sequence of tokens\n",
        "    self.constraints = constraints # list of constraints the sequence currently contains"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Jq1-dXz_wc4G"
      },
      "outputs": [],
      "source": [
        "# create a grid with dimensions of the max length of a sequence and the number of constraints + 1\n",
        "# (to account for when there are no constraints in the sequence)\n",
        "def init_grid(max_len, num_constraints):\n",
        "  grid = []\n",
        "  for i in range(max_len):\n",
        "    row = []\n",
        "    for j in range(num_constraints + 1):\n",
        "        row.append([])\n",
        "    grid.append(row)\n",
        "  return grid"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "constraint_boost = 11"
      ],
      "metadata": {
        "id": "MAAg6DTPJl6j"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "t_QwUcPnueJV"
      },
      "outputs": [],
      "source": [
        "# generate new open hypotheses\n",
        "def generate(model, hyp, input_ids, beam_size, encoder_outputs, constraint_boost=0, constraints=[]):\n",
        "\n",
        "  # check if sequence has EOS, return since no tokens can be added\n",
        "  if hyp.sequence[0, -1].item() == tokenizer.eos_token_id:\n",
        "    return [hyp]\n",
        "\n",
        "  decoder_input_ids = hyp.sequence\n",
        "\n",
        "  with torch.no_grad():\n",
        "    outputs = model(\n",
        "        input_ids=None,\n",
        "        encoder_outputs=encoder_outputs,\n",
        "        decoder_input_ids=decoder_input_ids\n",
        "    )\n",
        "    logits = outputs.logits\n",
        "\n",
        "  # get the logits for the last predicted token\n",
        "  next_token_logits = logits[:, -1, :] # [batch_size, sequence_length, vocab_size]\n",
        "  next_token_probs = torch.softmax(next_token_logits, dim=-1).squeeze(0)\n",
        "\n",
        "  # get the top beam_size tokens\n",
        "  top_token_probs, top_token_ids = torch.topk(next_token_probs, beam_size)\n",
        "\n",
        "  new_hyps = []\n",
        "\n",
        "  # create new hypotheses with the top beam_size tokens\n",
        "  for i in range(beam_size):\n",
        "    next_token_id = top_token_ids[i].item()\n",
        "    next_token_prob = top_token_probs[i].item()\n",
        "\n",
        "    decoded_token = tokenizer.decode(next_token_id, skip_special_tokens=False)\n",
        "\n",
        "    # check to see if the new token is a constraint and\n",
        "    # if that constraint is already in the sequence\n",
        "    flag = False\n",
        "    for constraint in hyp.constraints:\n",
        "      if decoded_token in constraint or decoded_token == constraint:\n",
        "        flag = True\n",
        "        continue\n",
        "\n",
        "    # if it is, skip the token\n",
        "    if flag:\n",
        "      continue\n",
        "\n",
        "    new_constraints = hyp.constraints\n",
        "    new_score = 0\n",
        "    is_open = True\n",
        "\n",
        "    # if the new token is a constraint and it is not in the sequence,\n",
        "    # add it to the constraint list\n",
        "    for constraint in constraints:\n",
        "      curr_constraint = constraint.split() if \" \" in constraint else [constraint]\n",
        "      if (decoded_token == curr_constraint[0]):\n",
        "        new_constraints = hyp.constraints + [decoded_token]\n",
        "        new_score = constraint_boost\n",
        "\n",
        "        # if the constraint is more than one word, close the hypothesis\n",
        "        # so the constraint can be continued\n",
        "        if len(curr_constraint) > 1:\n",
        "          is_open = False\n",
        "\n",
        "    new_score = new_score + hyp.score + next_token_prob # add the score of the token to the total score\n",
        "    new_sequence = torch.cat([decoder_input_ids, torch.tensor([[next_token_id]])], dim=-1) # add the token to the sequence\n",
        "\n",
        "    new_hyp = Hypothesis(new_score, is_open, new_sequence, new_constraints) # create a new hypothesis\n",
        "    new_hyps.append(new_hyp)\n",
        "\n",
        "  return new_hyps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "JqatlMs70IRh"
      },
      "outputs": [],
      "source": [
        "# start new constrained hypotheses\n",
        "def start(model, hyp, input_ids, constraints, encoder_outputs, constraint_boost):\n",
        "\n",
        "  # check if sequence has EOS, return since no tokens can be added\n",
        "  if hyp.sequence[0, -1].item() == tokenizer.eos_token_id:\n",
        "    return [hyp]\n",
        "\n",
        "  new_hyps = []\n",
        "\n",
        "  # get the first word of a multi word constraint or a single word constraint itself\n",
        "  for constraint in constraints:\n",
        "    first_word = constraint.split()[0] if ' ' in constraint else constraint\n",
        "    # if the constraint is already in the sequence, skip it\n",
        "    if first_word in hyp.constraints:\n",
        "      continue\n",
        "\n",
        "    first_word_token_ids = tokenizer.encode(first_word, add_special_tokens=False) # get the tokens of the word\n",
        "\n",
        "    new_sequence = hyp.sequence\n",
        "    log_prob = 0.0\n",
        "\n",
        "    # takes account for multi token words\n",
        "    for token_id in first_word_token_ids:\n",
        "      new_sequence = torch.cat([new_sequence, torch.tensor([[token_id]])], dim=-1) # add the token to the sequence\n",
        "\n",
        "      with torch.no_grad():\n",
        "        outputs = model(\n",
        "            input_ids=None,\n",
        "            encoder_outputs=encoder_outputs,\n",
        "            decoder_input_ids=new_sequence\n",
        "        )\n",
        "        logits = outputs.logits\n",
        "\n",
        "      # get the logits for the last predicted token\n",
        "      next_token_logits = logits[:, -1, :] # [batch_size, sequence_length, vocab_size]\n",
        "      log_probs = torch.log_softmax(next_token_logits, dim=-1).squeeze(0)\n",
        "\n",
        "      log_prob += log_probs[token_id].item() # add up the probs of each token\n",
        "\n",
        "    constraint_factor = len(hyp.constraints) if len(hyp.constraints) > 0 else 1 # normalization factor\n",
        "    # log_prob / len(first_word_token_ids) to take account of multi token constraints and\n",
        "    # constraint_boost / constraint_factor to make each word in the constraint equal\n",
        "    new_score = (hyp.score + (log_prob / len(first_word_token_ids))) + (constraint_boost / constraint_factor)\n",
        "\n",
        "    new_constraints = hyp.constraints + [first_word]\n",
        "    is_open = False if ' ' in constraint else True # single word constraints are finished, multi word constraints need to be continued\n",
        "\n",
        "    new_hyp = Hypothesis(new_score, is_open, new_sequence, new_constraints)\n",
        "    new_hyps.append(new_hyp)\n",
        "\n",
        "  return new_hyps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "aBjf1B_f0KB6"
      },
      "outputs": [],
      "source": [
        "# continue unfinished constraints\n",
        "def cont(model, hyp, input_ids, constraints, encoder_outputs, constraint_boost):\n",
        "\n",
        "  # check if sequence has EOS, return since no tokens can be added\n",
        "  if hyp.sequence[0, -1].item() == tokenizer.eos_token_id:\n",
        "    return hyp\n",
        "\n",
        "  for constraint in constraints:\n",
        "    # look at the last constraint added to the sequence\n",
        "    if (hyp.constraints[-1] in constraint):\n",
        "      constraint_words = constraint.split()\n",
        "      completed_words = [word for word in hyp.constraints if word in constraint_words] # get the completed words of the constraint\n",
        "\n",
        "      # check if the constraint is incomplete\n",
        "      if (len(completed_words) < len(constraint_words)):\n",
        "        next_word = constraint_words[len(completed_words)]\n",
        "        next_word_token_ids = tokenizer.encode(next_word, add_special_tokens=False)\n",
        "\n",
        "        new_sequence = hyp.sequence\n",
        "        log_prob = 0.0\n",
        "\n",
        "        # takes account for multi token words\n",
        "        for token_id in next_word_token_ids:\n",
        "          new_sequence = torch.cat([new_sequence, torch.tensor([[token_id]])], dim=-1) # add the token to the sequence\n",
        "\n",
        "          with torch.no_grad():\n",
        "            outputs = model(\n",
        "                input_ids=None,\n",
        "                encoder_outputs=encoder_outputs,\n",
        "                decoder_input_ids=new_sequence\n",
        "            )\n",
        "            logits = outputs.logits\n",
        "\n",
        "          # get the logits for the last predicted token\n",
        "          next_token_logits = logits[:, -1, :] # [batch_size, sequence_length, vocab_size]\n",
        "          log_probs = torch.log_softmax(next_token_logits, dim=-1).squeeze(0)\n",
        "\n",
        "          log_prob += log_probs[token_id].item() # add up the probs of each token\n",
        "\n",
        "        constraint_factor = len(hyp.constraints) if len(hyp.constraints) > 0 else 1 # normalization factor\n",
        "        # log_prob / len(first_word_token_ids) to take account of multi token constraints and\n",
        "        # constraint_boost / constraint_factor to make each word in the constraint equal\n",
        "        new_score = hyp.score + (log_prob / len(next_word_token_ids)) + (constraint_boost / constraint_factor)\n",
        "\n",
        "        new_constraints = hyp.constraints + [next_word]\n",
        "\n",
        "        completed_words.append(next_word)\n",
        "        is_open = len(completed_words) == len(constraint_words) # check if constraints are completed\n",
        "\n",
        "        new_hyp = Hypothesis(new_score, is_open, new_sequence, new_constraints)\n",
        "\n",
        "        return new_hyp\n",
        "\n",
        "  return hyp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "fcXMutx4Q8v2"
      },
      "outputs": [],
      "source": [
        "def constrained_beam_search(model, input_ids, constraints, max_len, num_constraints, constraint_boost=10, beam_size=5):\n",
        "\n",
        "  \"\"\"\n",
        "  model: model\n",
        "  input_ids: tokenized sentence\n",
        "  constraints: list of constraints\n",
        "  max_len: max length of translation\n",
        "  num_constraints: total number of words in the list of constraints\n",
        "  beam_size: beam size\n",
        "  \"\"\"\n",
        "  start_hyp = Hypothesis() # initial hypothesis\n",
        "  grid = init_grid(max_len, num_constraints) # initialize beams in grid\n",
        "  grid[0][0] = [start_hyp]\n",
        "\n",
        "  # forward pass\n",
        "  with torch.no_grad():\n",
        "    encoder_outputs = model.get_encoder()(input_ids=input_ids)\n",
        "\n",
        "  for t in range(1, max_len): # time step\n",
        "    for c in range(max(0, (num_constraints + t) - max_len), min(t, num_constraints) + 1): # num of constraints step\n",
        "      n, s, g = [], [], []\n",
        "\n",
        "      for hyp in grid[t - 1][c]:\n",
        "        if hyp.is_open:\n",
        "          # generate new hypotheses\n",
        "          new_hyps = generate(model, hyp, input_ids, beam_size, encoder_outputs, constraint_boost, constraints)\n",
        "          g.extend(new_hyps)\n",
        "\n",
        "      if c > 0:\n",
        "        for hyp in grid[t - 1][c - 1]:\n",
        "          if hyp.is_open:\n",
        "            # start new constraints\n",
        "            new_hyps = start(model, hyp, input_ids, constraints, encoder_outputs, constraint_boost)\n",
        "            s.extend(new_hyps)\n",
        "          else:\n",
        "            # continue unfinished constraints\n",
        "            new_hyp = cont(model, hyp, input_ids, constraints, encoder_outputs, constraint_boost)\n",
        "            n.append(new_hyp)\n",
        "\n",
        "      # sort the hypotheses from highest score to lowest score\n",
        "      all_hyps = sorted(n + s + g, key=lambda hyp: hyp.score, reverse=True)\n",
        "\n",
        "      valid_hyps = []\n",
        "      # valid hyps are hyps where the sequence has not ended or all the constraints are met\n",
        "      for hyp in all_hyps:\n",
        "        # where the sequence has not ended means it can still be expanded\n",
        "        if (hyp.sequence[0, -1].item() != tokenizer.eos_token_id):\n",
        "          valid_hyps.append(hyp)\n",
        "        else:\n",
        "          # where the sequence has ended and it has all the constraints means it's done\n",
        "          if (len(hyp.constraints) == num_constraints):\n",
        "            valid_hyps.append(hyp)\n",
        "\n",
        "      grid[t][c] = valid_hyps[:beam_size] # top beam_size scoring hypotheses stay on the beam\n",
        "\n",
        "  top_level_hyps = []\n",
        "  # get hyps in top level beams\n",
        "  for t in range(len(grid)):\n",
        "    top_level_hyps.extend(grid[t][num_constraints])\n",
        "\n",
        "  finished_hyps = []\n",
        "  # get hyps with EOS token\n",
        "  for hyp in top_level_hyps:\n",
        "    if hyp.sequence[0, -1].item() == tokenizer.eos_token_id:\n",
        "      finished_hyps.append(hyp)\n",
        "\n",
        "  # if no finished hyps\n",
        "  if not finished_hyps:\n",
        "    print(\"Warning: No finished hypotheses. Returning the best incomplete hypothesis.\")\n",
        "    best_hyp = max(top_level_hyps, key=lambda hyp: hyp.score)\n",
        "  else:\n",
        "    best_hyp = max(finished_hyps, key=lambda hyp: hyp.score) # get best hyp\n",
        "\n",
        "  return best_hyp\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_baseline_translation(model, input_ids):\n",
        "  hyp = Hypothesis()\n",
        "\n",
        "  # forward pass\n",
        "  with torch.no_grad():\n",
        "    encoder_outputs = model.get_encoder()(input_ids=input_ids)\n",
        "\n",
        "  while True:\n",
        "    hyp = generate(model, hyp, input_ids, 1, encoder_outputs)[0]\n",
        "\n",
        "    if hyp.sequence[0, -1].item() == tokenizer.eos_token_id:\n",
        "      break\n",
        "\n",
        "  return hyp.sequence"
      ],
      "metadata": {
        "id": "9DrDERSJu_mQ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en_sentences_dev = [row['translation']['en'] for row in de_en_dev]\n",
        "de_sentences_dev = [row['translation']['de'] for row in de_en_dev]"
      ],
      "metadata": {
        "id": "Kd_0hXC2_ZpQ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en_sentences_test = [row['translation']['en'] for row in de_en_test]\n",
        "de_sentences_test = [row['translation']['de'] for row in de_en_test]"
      ],
      "metadata": {
        "id": "ufLmde6tE0zi"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_constraints(reference, hypothesis, constraint_length=3):\n",
        "  ref_words = reference.split()\n",
        "  hyp_words = set(hypothesis.split())\n",
        "\n",
        "  constraints = []\n",
        "  used_indices = set()\n",
        "\n",
        "  # extract constraints of length constraint_length\n",
        "  for i in range(len(ref_words) - constraint_length + 1):\n",
        "    if any(idx in used_indices for idx in range(i, i + constraint_length)):\n",
        "      continue  # skip overlapping tokens\n",
        "\n",
        "    phrase = \" \".join(ref_words[i : i + constraint_length])\n",
        "    first_word = ref_words[i]\n",
        "\n",
        "    # only add the phrase if the first word is missing in the hypothesis (relaxed constraints)\n",
        "    if first_word not in hyp_words:\n",
        "      constraints.append(phrase)\n",
        "      used_indices.update(range(i, i + constraint_length))\n",
        "\n",
        "  # if no constraints of `constraint_length` are found, look for shorter phrases\n",
        "  if not constraints:\n",
        "    for length in range(constraint_length - 1, 0, -1):\n",
        "      for i in range(len(ref_words) - length + 1):\n",
        "        if any(idx in used_indices for idx in range(i, i + length)):\n",
        "          continue  # skip overlapping tokens\n",
        "\n",
        "        phrase = \" \".join(ref_words[i : i + length])\n",
        "        first_word = ref_words[i]\n",
        "\n",
        "        if first_word not in hyp_words:\n",
        "          constraints.append(phrase)\n",
        "          used_indices.update(range(i, i + length))\n",
        "\n",
        "      if constraints:\n",
        "          break\n",
        "\n",
        "  # handle remaining tokens at the end of the reference sentence\n",
        "  last_unprocessed_idx = max(used_indices) + 1 if used_indices else 0\n",
        "  if last_unprocessed_idx < len(ref_words):\n",
        "    remaining_phrase = \" \".join(ref_words[last_unprocessed_idx:])\n",
        "    first_word = ref_words[last_unprocessed_idx]\n",
        "\n",
        "    if first_word not in hyp_words:\n",
        "      constraints.append(remaining_phrase)\n",
        "\n",
        "  return constraints\n"
      ],
      "metadata": {
        "id": "4eNlT2PsiIAI"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "5E16b_upjqIi"
      },
      "outputs": [],
      "source": [
        "# compute score using BLEU\n",
        "def compute_BLEU_score(prediction, actual):\n",
        "  prediction_tokens = prediction.split()\n",
        "  actual_tokens = actual.split()\n",
        "\n",
        "  bleu_score = sentence_bleu(\n",
        "      [actual_tokens],  # reference\n",
        "      prediction_tokens,  # hypothesis\n",
        "  )\n",
        "\n",
        "  return bleu_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "best_constrained_score = -float(\"inf\")\n",
        "for beam_size in [5, 10, 15]:\n",
        "  for constraint_boost in [5, 10, 15]:\n",
        "    baseline_score_avg = 0\n",
        "    constrained_score_avg = 0\n",
        "    sentence_count = 0\n",
        "\n",
        "    for en_sentence, de_sentence in zip(en_sentences_dev[:100], de_sentences_dev[:100]):\n",
        "      input_ids = tokenize(en_sentence)\n",
        "\n",
        "      if len(en_sentence.split()) > 15:\n",
        "        continue\n",
        "\n",
        "      baseline_translation = decode(generate_baseline_translation(model, input_ids).flatten().tolist())\n",
        "\n",
        "      if baseline_translation == de_sentence:\n",
        "        constrained_translation = baseline_translation\n",
        "      else:\n",
        "        constraints = extract_constraints(de_sentence, baseline_translation)\n",
        "        num_constraints = sum(len(constraint.split()) if \" \" in constraint else 1 for constraint in constraints) if len(constraints) > 0 else 0\n",
        "        constrained_translation = decode(constrained_beam_search(model, input_ids, constraints, len(en_sentence.split()) + 10, num_constraints, constraint_boost, beam_size).sequence.flatten().tolist())\n",
        "\n",
        "      baseline_score = compute_BLEU_score(baseline_translation, de_sentence)\n",
        "      constrained_score = compute_BLEU_score(constrained_translation, de_sentence)\n",
        "\n",
        "      sentence_count += 1\n",
        "\n",
        "      print(sentence_count)\n",
        "      print(baseline_translation, baseline_score)\n",
        "      print(constrained_translation, constrained_score)\n",
        "      print()\n",
        "\n",
        "      baseline_score_avg += baseline_score\n",
        "      constrained_score_avg += constrained_score\n",
        "\n",
        "    baseline_score_avg /= sentence_count\n",
        "    constrained_score_avg /= sentence_count\n",
        "\n",
        "    results.append({\n",
        "                \"beam_size\": beam_size,\n",
        "                \"constraint_boost\": constraint_boost,\n",
        "                \"baseline_score_avg\": baseline_score_avg,\n",
        "                \"constrained_score_avg\": constrained_score_avg,\n",
        "                \"sentence_count\": sentence_count\n",
        "            })\n",
        "\n",
        "    # Check if this is the best score so far\n",
        "    if constrained_score_avg > best_constrained_score:\n",
        "        best_constrained_score = constrained_score_avg\n",
        "        best_params = (beam_size, constraint_boost)\n",
        "\n",
        "    print(f\"Beam size: {beam_size}, Boost: {constraint_boost}\")\n",
        "    print(f\"Baseline Avg BLEU: {baseline_score_avg}, Constrained Avg BLEU: {constrained_score_avg}, Sentences: {sentence_count}\")\n",
        "\n",
        "  print(\"\\nBest Parameters:\")\n",
        "  print(f\"Beam size: {best_params[0]}, Constraint boost: {best_params[1]}\")\n",
        "  print(f\"Best Constrained BLEU: {best_constrained_score}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OdRHtliz_tFR",
        "outputId": "f2ab17de-6fff-4f8d-ddca-e4a86b222fdf"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "28-jähriger Chef fand tot in San Francisco Mall 0.38260294162784475\n",
            "Der 28-jährige Koch in San Francisco Mall wurde tot gefunden. 0.4111336169005197\n",
            "\n",
            "2\n",
            "Ein Sprecher von Sons & Daughters sagte, sie seien \"geschockt und verwüstet\" durch seinen Tod. 0.1772712285241271\n",
            "Ein Sprecher von dass sie über Son am Boden zerstört Tod \"schockiert und verwisch des Sons & Daughter seien\". 6.742419825730601e-78\n",
            "\n",
            "3\n",
            "Unsere Gedanken und unser Beileid sind mit Franks Familie und Freunden in dieser schwierigen Zeit. 0.4480304273880272\n",
            "Unsere Gedanken und unser Beileid sind mit Franks Familie und Freunden in dieser schweren Zeit bei... 0.537284965911771\n",
            "\n",
            "Warning: No finished hypotheses. Returning the best incomplete hypothesis.\n",
            "4\n",
            "\"Er hat eine Wohnung gefunden, er ist mit einem Mädchen zusammen\", sagte Louis Galicia zu KGO. 1.2882297539194154e-231\n",
            "\"Er hat eine Wohnung gefunden hatte eine eine Wohnung, er war mit einem Mädchen fand 3.6170146665513074e-78\n",
            "\n",
            "5\n",
            "Er war ein freundlicher Geist mit einem großen Herzen. 0.5969491792019646\n",
            "Er war ein freundlicher Geist mit einem großen Herzen. Mensch mit einem großen Herzen. 0.44534504264163466\n",
            "\n",
            "6\n",
            "Er wollte nie in irgendeiner Art von Auseinandersetzung sein. 0.43167001068522526\n",
            "Er wollte nie in irgendeiner Art von Auseinandersetzung sein.. an irgendeiner Art. 0.33180774028439425\n",
            "\n",
            "7\n",
            "Er war der Bruder, der mit dem Fluss ging. 0.7259795291154771\n",
            "Er war der Bruder, der mit dem Strom schwamm. 1.0\n",
            "\n",
            "8\n",
            "Jeder, der Informationen hat, wird gebeten, die SFPD Tip Line unter 415-575-4444 anzurufen. 3.1640699269154553e-78\n",
            "gebeten das Hinweistelefon an des SFPD unter zu dem Fall 415-575-4444. Nummer 415-575-4444 anzurufen. 5.9432174125942455e-78\n",
            "\n",
            "9\n",
            "Jennifer Aniston: Ich bin immer geplatzt 4.446808895758207e-78\n",
            "Jennifer Anis werde immer ins Schubladen gesteckt. 7.183445846156676e-155\n",
            "\n",
            "10\n",
            "Jennifer Aniston muss nicht immer perfekt oder erfolgreich sein. 1.0\n",
            "Jennifer Aniston muss nicht immer perfekt oder erfolgreich sein. 1.0\n",
            "\n",
            "11\n",
            "Das hat der Hollywood-Star in einem Interview deutlich gemacht. 0.6010525952194526\n",
            "Das hat der Hollywood-Star in einem Interview reichlich deutlich gemacht. . . jetzt unmissverständlich klar. 0.4546697236991713\n",
            "\n",
            "12\n",
            "\"Persönlich habe ich nicht den Wunsch, die ganze Zeit perfekt und erfolgreich zu sein.\" 0.3661926362999429\n",
            "\"Persönlich wünsche \"Ich selbst habe nicht immer perfekt und erfolgreich zu sein.\". 0.53107253497887\n",
            "\n",
            "13\n",
            "\"Um ehrlich zu sein, mir ist das so was nicht wirklich wichtig.\" 9.788429383461836e-232\n",
            "\"Um ehrlich zu sein, es interessiert mich nicht wirklich einen Scheiß diese Dinge eher zu \"Ehrlich gesagt, sind schnuppe.\" 3.826170841813515e-78\n",
            "\n",
            "14\n",
            "August in Deutschland veröffentlicht. 0\n",
            "bei uns ab er in Deutschland auf dem 25. August. Der Film läuft am 6.373848007830757e-78\n",
            "\n",
            "15\n",
            "Golfer Langer wird mit der Sportpyramide ausgezeichnet 7.711523862191631e-155\n",
            "Golfer Langer wird mit der Sportpyramide geehrt erhält die Sportpyramide 4.640083527732319e-78\n",
            "\n",
            "16\n",
            "Seine Erfahrung auf dem Pferd ist vernachlässigbar. 5.395774370246974e-78\n",
            "Seine Erfahrungen auf dem Pferd sind vernachlässigbar. 0.8091067115702212\n",
            "\n",
            "17\n",
            "Es war das erste für den 58-Jährigen. 1.331960397810445e-231\n",
            "Das erste es eine Premiere. Für den 58-Jährigen! 5.614021910443866e-78\n",
            "\n",
            "18\n",
            "CHIO: \"Goldene Sportpyramide\" für Bernhard Langer 1.0\n",
            "CHIO: \"Goldene Sportpyramide\" für Bernhard Langer 1.0\n",
            "\n",
            "19\n",
            "Nach einem Kilometer bei dieser Geschwindigkeit hatte ich Angst. 6.7393716283177006e-155\n",
            "Nach einem habe Angst gehabt. Nach diesem Tempo, ich Ein Kilometer bei 7.2288860455763025e-78\n",
            "\n",
            "20\n",
            "\"Es war keine gute Erfahrung\", sagte Langer. 7.711523862191631e-155\n",
            "kein gutes Erlebnis\", sagt Langer. 0.6703200460356393\n",
            "\n",
            "21\n",
            "Und damit endete seine Karriere im Reiten wieder. 4.2338646970116024e-78\n",
            "die Reitkarriere dannte ging damit wieder zu Ende. Und so ging auch schon wieder. 3.8245273678543377e-78\n",
            "\n",
            "22\n",
            "Langers erstes Mal in Aachen und bei CHIO. 0.22172045047934608\n",
            "erste Mal in Aachen und am CH Langer war das damit auch beim CHIO. 0.5333505353503044\n",
            "\n",
            "23\n",
            "Er war völlig überzeugt. 1.0032743411283238e-231\n",
            "Der Grund war ihm durchaus überzeugend. 7.262123179505913e-78\n",
            "\n",
            "24\n",
            "Der freundliche Sportler fehlt nicht für Auszeichnungen. 9.709385502639237e-232\n",
            "Der freundliche Athlet An Auszeichnungen mangelt es dem sympathischen Sportler nicht aus. 0.5156626918239822\n",
            "\n",
            "25\n",
            "Das Bundesverdienstkreuz, das Silber Laurel Blatt, Champions Tour Player of the Year... 5.791739854583281e-155\n",
            "Das Bundesverdienstkreuz, die Silber Silbernes Lorbeerblatt, Champions-Tour-Player des Jahres... 4.335118471269586e-78\n",
            "\n",
            "26\n",
            "Sogar die britische Königin hat ihm eine Ehre erwiesen. 6.7393716283177006e-155\n",
            "schon geadelt. Selbst die britische Queen hat ihn eine Ehre erwiesen! 0.5193071778680676\n",
            "\n",
            "27\n",
            "Langer ist die 18. Person, die mit der Sportpyramide ausgezeichnet wird. 6.513869329968086e-155\n",
            "Langer ist mit der Sportpyramide der 18. Preisträger der Sportpyramide. 0.4518010018049224\n",
            "\n",
            "28\n",
            "Am Samstag traf er den ersten Menschen, der ihn in Aachen empfing, Hans Günter Winkler. 4.13876789251507e-78\n",
            "Am Samstag traf er die erste Person, am Samstag den Hans Güner Winkl Preisträger Hans Günter In Aachen traf. 4.629271609155891e-78\n",
            "\n",
            "29\n",
            "Langer fördert seit Jahren Nachwuchstalente. 1.1862177682648818e-231\n",
            "Langer fördert das Nachwuchstalent seit Jahren Seit Jahrzehnten fördert. - Was? 6.513869329968086e-155\n",
            "\n",
            "30\n",
            "Und in dieser Hinsicht hat er sofort das Preisgeld von 25.000 Euro übergeben. 0.31455601883230705\n",
            "Auf diese Weise übergab er sofort das Preisgeld von 25.000 Euro. auch das Preisgeld so gab er. 0.2140909265975804\n",
            "\n",
            "31\n",
            "Viele dieser Sportlegenden kamen am Samstag nach Aachen. 6.626356351409518e-78\n",
            "Viele dieser Sportlegenden kamen samstags nach Aachen und waren am Samstag. 5.008605395783359e-78\n",
            "\n",
            "32\n",
            "Bernhard Langer hielt sich von den großen Tieren fern. 5.072841446586652e-78\n",
            "Bernhard Langer hielt Abstand zu den großen Tieren. 1.0\n",
            "\n",
            "33\n",
            "Vergleich der Wirtschaftspolitik von Clinton und Trump: Es geht um das Geld 1.1640469867513693e-231\n",
            "all about the Geld im Vergleich: It's ongeround money and Trump Clintons und Trumps Wirtschaftspolitik 0.31138788080750657\n",
            "\n",
            "34\n",
            "Sowohl Clinton als auch Trump wollen in Infrastruktur investieren und das TTP-Handelsabkommen verhindern. 0.229585358002991\n",
            "sowohl Clinton alsen Trump In Infrastruktur investieren und verhindern das Handelsabkommen TTP verhindern. - das wollen ist. 0.29213008358451265\n",
            "\n",
            "35\n",
            "Das ist die Rolle des Staates. 0.5081327481546147\n",
            "Dies ist die Rolle des Staates Der, über die wir sprechen. 5.475733548646979e-78\n",
            "\n",
            "36\n",
            "Er würde die Dinge ganz anders machen, sagt Trump. 1.384292958842266e-231\n",
            "anders, behauptet Trump. selber dagegen wäre die Dinge ganz und gar nicht tun! 5.191033047549162e-78\n",
            "\n",
            "37\n",
            "Das würde die Steuern für die Reichen erhöhen, sagt sie. 0.4518010018049224\n",
            "Dies würde die Steuern für die Reichen erhöhen Deshalb würden die erhöht, sagt sie. 0.42718025135819776\n",
            "\n",
            "38\n",
            "Interessanterweise haben sie ähnliche Ansichten über die Infrastruktur. 1.1368587676511996e-231\n",
            "sehr nahe beim sind sich beidet haben. Thema Infrastruktur. 5.929498746342409e-78\n",
            "\n",
            "39\n",
            "Es muss Investitionen in Infrastruktur geben, sagen Trump und Clinton fast wörtlich. 0.31702331385234306\n",
            "Die In die müsse Infrastruktur muss investiert werden, sagen Trump und Clinton fast wörtlich. 0.5316967153331754\n",
            "\n",
            "40\n",
            "Trump könnte daher zu Diskussionen innerhalb der Partei führen. 1.2508498911928379e-231\n",
            "Trump könnte also noch parteiinterne Diskussionen innerhalb der Partei herbeiführen Auf Trump könnten. 0.22997519112894443\n",
            "\n",
            "41\n",
            "Es gibt klarere Unterschiede in der Energie- und Klimapolitik. 0.5873949094699213\n",
            "Bei der Energie- und Klimapolitik sind klarere Unterschiede zu erkennen. sichtigen Deutlicher sind die 4.887310777383362e-78\n",
            "\n",
            "42\n",
            "Auch nicht wollen TTP unterzeichnen, das Transpazifische Handelsabkommen. 1.4256605770826504e-231\n",
            "Handelsabkommen, wollen beide TTP auch nicht unterzeichnen. Das trans TPP, das transpazifische 6.0598953153840875e-78\n",
            "\n",
            "43\n",
            "Dann sollten sie auch besser bezahlt werden, fordert Clinton. 0.4180134288483488\n",
            "Die sollen dann sollten auch besser bezahlt werden, fordert Clinton. 0.5410822690539396\n",
            "\n",
            "44\n",
            "Der derzeitige Mindestlohn von 7,25 US-Dollar sei eine Schwäche, sagt sie. 0.5706745777055999\n",
            "Der derzeitige Mindestlohn von 7,25 US-Dollar sei ein Hungerlohn. 1.0\n",
            "\n",
            "45\n",
            "Sie will es auf 15 Dollar pro Stunde erhöhen. 0.6104735835807844\n",
            "Sie will es auf 15 Dollar pro Stunde erhöhen. Sie Ihn will sie. 0.42803206067505944\n",
            "\n",
            "46\n",
            "Seine Kritiker erklären, dass dies das Haushaltsdefizit nur erhöhen wird. 3.798979081005021e-78\n",
            "Seine Kritiker geben an, dass die halten ihm vor, das Haushaltsdefizit zu erhöhen. damit würde das 5.171389238907095e-78\n",
            "\n",
            "47\n",
            "Zuflucht suchen: \"Jede Flüchtlingin hat sich mit sexueller Gewalt beschäftigt\" 6.8489908526642754e-155\n",
            "Refugium suchen: \"Jeder Flüchtling hat Frau hat Erfahrungen Flucht: \"Jede geflüchtete 5.731095145962094e-78\n",
            "\n",
            "48\n",
            "Wenn sie in Deutschland ankommen, sind sie oft traumatisiert. 1.0\n",
            "Wenn sie in Deutschland ankommen, sind sie oft traumatisiert. 1.0\n",
            "\n",
            "49\n",
            "Ein Haus bietet Zuflucht. 1.0\n",
            "Ein Haus bietet Zuflucht. 1.0\n",
            "\n",
            "50\n",
            "Vier Frauen sitzen in einer großen Küche. 7.711523862191631e-155\n",
            "In einer großenem Küche sitzen vier Frauen Frauen. 5.87583260478785e-78\n",
            "\n",
            "51\n",
            "Eine Frau hält ihren kleinen Sohn in den Armen. 0.4671379777282001\n",
            "Eine Frau hält ihren kleinen Sohn in den Armen. Sie hat ihren kleinen. 0.33260249505555045\n",
            "\n",
            "52\n",
            "Er schaut neugierig um den Raum, seine Augen weit. 1.4875195904069663e-231\n",
            "Er schaut neugierig um den Raum, in den Raum. guckt mit großenten Augen, 4.0622028886850106e-78\n",
            "\n",
            "53\n",
            "Seine Mutter starrt auf die Tischplatte. 1.0\n",
            "Seine Mutter starrt auf die Tischplatte. 1.0\n",
            "\n",
            "54\n",
            "Bis sie merkt, dass sie beobachtet wird. 1.0\n",
            "Bis sie merkt, dass sie beobachtet wird. 1.0\n",
            "\n",
            "55\n",
            "Sie lächelt kurz und küsst das Kind. 0.6147881529512643\n",
            "Sie lächelt kurz und küsst das Kind auf den Kleinen. 0.4518010018049224\n",
            "\n",
            "56\n",
            "Die Szene sieht normal aus. 8.38826642100846e-155\n",
            "Die Szene wirkt normal. 1.0\n",
            "\n",
            "Beam size: 5, Boost: 5\n",
            "Baseline Avg BLEU: 0.26859017981201255, Constrained Avg BLEU: 0.35662215471908726, Sentences: 56\n",
            "1\n",
            "28-jähriger Chef fand tot in San Francisco Mall 0.38260294162784475\n",
            "Der 28-jährige Koch in San Francisco Mall wurde tot gefunden. 0.4111336169005197\n",
            "\n",
            "2\n",
            "Ein Sprecher von Sons & Daughters sagte, sie seien \"geschockt und verwüstet\" durch seinen Tod. 0.1772712285241271\n",
            "Tod \"schockiert und am Boden zerstört, dass sie über die Grenzen des Sons & Daughter- seien\". 0.3397898021621042\n",
            "\n",
            "3\n",
            "Unsere Gedanken und unser Beileid sind mit Franks Familie und Freunden in dieser schwierigen Zeit. 0.4480304273880272\n",
            "Unsere Gedanken und unser Beileid sind mit Franks Familie und Freunden in dieser schweren Zeit beibachtet. 0.537284965911771\n",
            "\n",
            "Warning: No finished hypotheses. Returning the best incomplete hypothesis.\n",
            "4\n",
            "\"Er hat eine Wohnung gefunden, er ist mit einem Mädchen zusammen\", sagte Louis Galicia zu KGO. 1.2882297539194154e-231\n",
            "\"Er hat eine Wohnung gefunden hatte eine eine Wohnung, er war mit fand 4.0622028886850106e-78\n",
            "\n",
            "5\n",
            "Er war ein freundlicher Geist mit einem großen Herzen. 0.5969491792019646\n",
            "Er war ein gütiger Mensch mit einem großen Herzen. 0.5969491792019646\n",
            "\n",
            "6\n",
            "Er wollte nie in irgendeiner Art von Auseinandersetzung sein. 0.43167001068522526\n",
            "Er wollte nie in irgendeiner Art von Auseinandersetzung sein.. an irgendeiner Art. 0.33180774028439425\n",
            "\n",
            "7\n",
            "Er war der Bruder, der mit dem Fluss ging. 0.7259795291154771\n",
            "Er war der Bruder, der mit dem Strom schwamm. 1.0\n",
            "\n",
            "8\n",
            "Jeder, der Informationen hat, wird gebeten, die SFPD Tip Line unter 415-575-4444 anzurufen. 3.1640699269154553e-78\n",
            "Wer Informationen hat, wird gebeten das Hinweistelefon zu dem Fall an des SFPD unter Nummer 415-575-4444 anzurufen. 0.3972595463783053\n",
            "\n",
            "9\n",
            "Jennifer Aniston: Ich bin immer geplatzt 4.446808895758207e-78\n",
            "Jennifer Anis werde immer ins Schubladen gesteckt. 7.183445846156676e-155\n",
            "\n",
            "10\n",
            "Jennifer Aniston muss nicht immer perfekt oder erfolgreich sein. 1.0\n",
            "Jennifer Aniston muss nicht immer perfekt oder erfolgreich sein. 1.0\n",
            "\n",
            "11\n",
            "Das hat der Hollywood-Star in einem Interview deutlich gemacht. 0.6010525952194526\n",
            "Das hat der Hollywood-Star in einem Interview reichlich deutlich gemacht. . . jetzt unmissverständlich klar. 0.4546697236991713\n",
            "\n",
            "12\n",
            "\"Persönlich habe ich nicht den Wunsch, die ganze Zeit perfekt und erfolgreich zu sein.\" 0.3661926362999429\n",
            "\"Persönlich wünsche \"Ich selbst habe nicht immer perfekt und erfolgreich zu sein.\". 0.53107253497887\n",
            "\n",
            "13\n",
            "\"Um ehrlich zu sein, mir ist das so was nicht wirklich wichtig.\" 9.788429383461836e-232\n",
            "\"Um ehrlich zu sein, es interessiert mich nicht wirklich einen Scheiß diese Dinge eher zu \"Ehrlich gesagt, sind schnuppe.\" 3.826170841813515e-78\n",
            "\n",
            "14\n",
            "August in Deutschland veröffentlicht. 0\n",
            "bei uns ab er in Deutschland auf dem 25. August. Der Film läuft am 6.373848007830757e-78\n",
            "\n",
            "15\n",
            "Golfer Langer wird mit der Sportpyramide ausgezeichnet 7.711523862191631e-155\n",
            "Golfer Langer wird mit der Sportpyramide geehrt erhält die Sportpyramide 4.640083527732319e-78\n",
            "\n",
            "16\n",
            "Seine Erfahrung auf dem Pferd ist vernachlässigbar. 5.395774370246974e-78\n",
            "Seine Erfahrungen auf dem Pferdeweg sind vernachlässigbar. 0.4347208719449914\n",
            "\n",
            "17\n",
            "Es war das erste für den 58-Jährigen. 1.331960397810445e-231\n",
            "Für den 58-Jährigen war es eine Premiere. 1.0\n",
            "\n",
            "18\n",
            "CHIO: \"Goldene Sportpyramide\" für Bernhard Langer 1.0\n",
            "CHIO: \"Goldene Sportpyramide\" für Bernhard Langer 1.0\n",
            "\n",
            "19\n",
            "Nach einem Kilometer bei dieser Geschwindigkeit hatte ich Angst. 6.7393716283177006e-155\n",
            "Mit diesem Tempo, ich habe Angst gehabt. - Ein Kilometer beiahnten mich! 0.46924700641055994\n",
            "\n",
            "20\n",
            "\"Es war keine gute Erfahrung\", sagte Langer. 7.711523862191631e-155\n",
            "kein gutes Erlebnis\", sagt Langer. 0.6703200460356393\n",
            "\n",
            "21\n",
            "Und damit endete seine Karriere im Reiten wieder. 4.2338646970116024e-78\n",
            "Und damit ging die Reitkarriere dann wieder zu Ende. Und das war auch schon wieder. 5.053216933905455e-78\n",
            "\n",
            "22\n",
            "Langers erstes Mal in Aachen und bei CHIO. 0.22172045047934608\n",
            "damit auch beim Langer war das erste Mal in Aachen. In der CHIO. 0.49735673561245436\n",
            "\n",
            "23\n",
            "Er war völlig überzeugt. 1.0032743411283238e-231\n",
            "Der Grund war ihm durchaus überzeugend. 7.262123179505913e-78\n",
            "\n",
            "24\n",
            "Der freundliche Sportler fehlt nicht für Auszeichnungen. 9.709385502639237e-232\n",
            "Der freundliche Athlet An Auszeichnungen mangelt es dem sympathischen Sportler nicht aus. 0.5156626918239822\n",
            "\n",
            "25\n",
            "Das Bundesverdienstkreuz, das Silber Laurel Blatt, Champions Tour Player of the Year... 5.791739854583281e-155\n",
            "Das Bundesverdienstkreuz, die Silber Silbernes Lorbeerblatt, Champions-Tour-Player des Jahres... 4.335118471269586e-78\n",
            "\n",
            "26\n",
            "Sogar die britische Königin hat ihm eine Ehre erwiesen. 6.7393716283177006e-155\n",
            "schon geadelt. Selbst die britische Queen hat ihn eine Ehre zuteil werden lassen! 0.42803206067505944\n",
            "\n",
            "27\n",
            "Langer ist die 18. Person, die mit der Sportpyramide ausgezeichnet wird. 6.513869329968086e-155\n",
            "Langer ist mit der Sportpyramide der 18. Preisträger der Sportpyramide. 0.4518010018049224\n",
            "\n",
            "28\n",
            "Am Samstag traf er den ersten Menschen, der ihn in Aachen empfing, Hans Günter Winkler. 4.13876789251507e-78\n",
            "In Aachen traf er am Samstag den ersten Empfänger, Hans Gündner Winkler. Preisträger Hans Günter 0.5828233954152654\n",
            "\n",
            "29\n",
            "Langer fördert seit Jahren Nachwuchstalente. 1.1862177682648818e-231\n",
            "Seit Jahrzehnten fördert Langers Nachwuchstalent an. - Was? 4.4646672960328985e-78\n",
            "\n",
            "30\n",
            "Und in dieser Hinsicht hat er sofort das Preisgeld von 25.000 Euro übergeben. 0.31455601883230705\n",
            "Und so gab er das Preisgeld in Höhe von 25.000 Euro sofort weiter. auch das Preisgeld. 0.2696698152099016\n",
            "\n",
            "31\n",
            "Viele dieser Sportlegenden kamen am Samstag nach Aachen. 6.626356351409518e-78\n",
            "Viele dieser Sportlegenden kamen samstags nach Aachen und waren am Samstag. 5.008605395783359e-78\n",
            "\n",
            "32\n",
            "Bernhard Langer hielt sich von den großen Tieren fern. 5.072841446586652e-78\n",
            "Bernhard Langer hielt Abstand zu den großen Tieren. 1.0\n",
            "\n",
            "33\n",
            "Vergleich der Wirtschaftspolitik von Clinton und Trump: Es geht um das Geld 1.1640469867513693e-231\n",
            "all about the Geld im Vergleich: It's ongeround money and Trump Clintons und Trumps Wirtschaftspolitik 0.31138788080750657\n",
            "\n",
            "34\n",
            "Sowohl Clinton als auch Trump wollen in Infrastruktur investieren und das TTP-Handelsabkommen verhindern. 0.229585358002991\n",
            "sowohl Clinton alsen Trump In Infrastruktur investieren wollen und verhindern das Handelsabkommen TTP verhindern. - das wollen. 5.400726898975435e-78\n",
            "\n",
            "35\n",
            "Das ist die Rolle des Staates. 0.5081327481546147\n",
            "Dies ist die Rolle des Staates Der, über die wir sprechen. 5.475733548646979e-78\n",
            "\n",
            "36\n",
            "Er würde die Dinge ganz anders machen, sagt Trump. 1.384292958842266e-231\n",
            "anders, behauptet Trump. selber dagegen wäre die Dinge ganz und gar nicht tun! 5.191033047549162e-78\n",
            "\n",
            "37\n",
            "Das würde die Steuern für die Reichen erhöhen, sagt sie. 0.4518010018049224\n",
            "Dies würde die Steuern für die Reichen erhöhen Deshalb würden die erhöht, sagt sie. 0.42718025135819776\n",
            "\n",
            "38\n",
            "Interessanterweise haben sie ähnliche Ansichten über die Infrastruktur. 1.1368587676511996e-231\n",
            "sehr nahe beim sind sich beidet haben. Thema Infrastruktur. 5.929498746342409e-78\n",
            "\n",
            "39\n",
            "Es muss Investitionen in Infrastruktur geben, sagen Trump und Clinton fast wörtlich. 0.31702331385234306\n",
            "Die In die müsse der Infrastruktur muss investiert werden, sagen Trump und Clinton fast wörtlich. 0.4920274515385508\n",
            "\n",
            "40\n",
            "Trump könnte daher zu Diskussionen innerhalb der Partei führen. 1.2508498911928379e-231\n",
            "Auf Trump könnten also noch parteiinterne Diskussionen innerhalb der Partei geführt werden. - Was? 0.4324227075463215\n",
            "\n",
            "41\n",
            "Es gibt klarere Unterschiede in der Energie- und Klimapolitik. 0.5873949094699213\n",
            "Bei der Energie- und Klimapolitik sind klarere Unterschiede zu erkennen. sichtigen Deutlicher sind die 4.887310777383362e-78\n",
            "\n",
            "42\n",
            "Auch nicht wollen TTP unterzeichnen, das Transpazifische Handelsabkommen. 1.4256605770826504e-231\n",
            "Handelsabkommen, wollen beide TTP auch nicht unterzeichnen. Das trans TPP, das transpazifische 6.0598953153840875e-78\n",
            "\n",
            "43\n",
            "Dann sollten sie auch besser bezahlt werden, fordert Clinton. 0.4180134288483488\n",
            "Die sollen dann sollten auch besser bezahlt werden, fordert Clinton. 0.5410822690539396\n",
            "\n",
            "44\n",
            "Der derzeitige Mindestlohn von 7,25 US-Dollar sei eine Schwäche, sagt sie. 0.5706745777055999\n",
            "Der derzeitige Mindestlohn von 7,25 US-Dollar sei ein Hungerlohn. 1.0\n",
            "\n",
            "45\n",
            "Sie will es auf 15 Dollar pro Stunde erhöhen. 0.6104735835807844\n",
            "Sie will es auf 15 Dollar pro Stunde erhöhen. Sie Ihn will sie. 0.42803206067505944\n",
            "\n",
            "46\n",
            "Seine Kritiker erklären, dass dies das Haushaltsdefizit nur erhöhen wird. 3.798979081005021e-78\n",
            "Seine Kritiker geben an, dass die halten ihm vor, das Haushaltsdefizit zu erhöhen. damit würde das 5.171389238907095e-78\n",
            "\n",
            "47\n",
            "Zuflucht suchen: \"Jede Flüchtlingin hat sich mit sexueller Gewalt beschäftigt\" 6.8489908526642754e-155\n",
            "Flucht: \"Jede geflüchtete Frau hat Erfahrungen über sexuelle Gewalt gemacht.\" 0.5169731539571706\n",
            "\n",
            "48\n",
            "Wenn sie in Deutschland ankommen, sind sie oft traumatisiert. 1.0\n",
            "Wenn sie in Deutschland ankommen, sind sie oft traumatisiert. 1.0\n",
            "\n",
            "49\n",
            "Ein Haus bietet Zuflucht. 1.0\n",
            "Ein Haus bietet Zuflucht. 1.0\n",
            "\n",
            "50\n",
            "Vier Frauen sitzen in einer großen Küche. 7.711523862191631e-155\n",
            "In einer großenem Küche sitzen vier Frauen sitzt. Frauen. 5.30941411456236e-78\n",
            "\n",
            "51\n",
            "Eine Frau hält ihren kleinen Sohn in den Armen. 0.4671379777282001\n",
            "Eine Frau hält ihren kleinen Sohn in den Armen. Sie hat ihren kleinen. 0.33260249505555045\n",
            "\n",
            "52\n",
            "Er schaut neugierig um den Raum, seine Augen weit. 1.4875195904069663e-231\n",
            "Er sieht in den Raum. Seine Augen sind guckt mit großenten Augen! 4.337579510280189e-78\n",
            "\n",
            "53\n",
            "Seine Mutter starrt auf die Tischplatte. 1.0\n",
            "Seine Mutter starrt auf die Tischplatte. 1.0\n",
            "\n",
            "54\n",
            "Bis sie merkt, dass sie beobachtet wird. 1.0\n",
            "Bis sie merkt, dass sie beobachtet wird. 1.0\n",
            "\n",
            "55\n",
            "Sie lächelt kurz und küsst das Kind. 0.6147881529512643\n",
            "Sie lächelt kurz und küsst das Kind auf den Kleinen. 0.4518010018049224\n",
            "\n",
            "56\n",
            "Die Szene sieht normal aus. 8.38826642100846e-155\n",
            "Die Szene wirkt normal. 1.0\n",
            "\n",
            "Beam size: 5, Boost: 10\n",
            "Baseline Avg BLEU: 0.26859017981201255, Constrained Avg BLEU: 0.4080912501115553, Sentences: 56\n",
            "1\n",
            "28-jähriger Chef fand tot in San Francisco Mall 0.38260294162784475\n",
            "Der 28-jährige Koch in San Francisco Mall wurde tot gefunden. 0.4111336169005197\n",
            "\n",
            "2\n",
            "Ein Sprecher von Sons & Daughters sagte, sie seien \"geschockt und verwüstet\" durch seinen Tod. 0.1772712285241271\n",
            "Tod \"schockiert und am Boden zerstört, dass sie über die Grenzen des Sons & Daughter- seien\". 0.3397898021621042\n",
            "\n",
            "3\n",
            "Unsere Gedanken und unser Beileid sind mit Franks Familie und Freunden in dieser schwierigen Zeit. 0.4480304273880272\n",
            "Unsere Gedanken und unser Beileid sind mit Franks Familie und Freunden in dieser schweren Zeit beibachtet. 0.537284965911771\n",
            "\n",
            "Warning: No finished hypotheses. Returning the best incomplete hypothesis.\n",
            "4\n",
            "\"Er hat eine Wohnung gefunden, er ist mit einem Mädchen zusammen\", sagte Louis Galicia zu KGO. 1.2882297539194154e-231\n",
            "hatte eine eine Wohnung, fand 2.532465547254365e-155\n",
            "\n",
            "5\n",
            "Er war ein freundlicher Geist mit einem großen Herzen. 0.5969491792019646\n",
            "Er war ein gütiger Mensch mit einem großen Herzen. 0.5969491792019646\n",
            "\n",
            "6\n",
            "Er wollte nie in irgendeiner Art von Auseinandersetzung sein. 0.43167001068522526\n",
            "Er wollte nie in irgendeiner Art von Auseinandersetzung sein.. an irgendeiner Art. 0.33180774028439425\n",
            "\n",
            "7\n",
            "Er war der Bruder, der mit dem Fluss ging. 0.7259795291154771\n",
            "Er war der Bruder, der mit dem Strom schwamm. 1.0\n",
            "\n",
            "8\n",
            "Jeder, der Informationen hat, wird gebeten, die SFPD Tip Line unter 415-575-4444 anzurufen. 3.1640699269154553e-78\n",
            "Wer Informationen hat, wird gebeten das Hinweistelefon zu dem Fall an des SFPD unter Nummer 415-575-4444 anzurufen. 0.3972595463783053\n",
            "\n",
            "9\n",
            "Jennifer Aniston: Ich bin immer geplatzt 4.446808895758207e-78\n",
            "Jennifer Anis werde immer ins Schubladen gesteckt. 7.183445846156676e-155\n",
            "\n",
            "10\n",
            "Jennifer Aniston muss nicht immer perfekt oder erfolgreich sein. 1.0\n",
            "Jennifer Aniston muss nicht immer perfekt oder erfolgreich sein. 1.0\n",
            "\n",
            "11\n",
            "Das hat der Hollywood-Star in einem Interview deutlich gemacht. 0.6010525952194526\n",
            "Das hat der Hollywood-Star in einem Interview reichlich deutlich gemacht. . . jetzt unmissverständlich klar. 0.4546697236991713\n",
            "\n",
            "12\n",
            "\"Persönlich habe ich nicht den Wunsch, die ganze Zeit perfekt und erfolgreich zu sein.\" 0.3661926362999429\n",
            "\"Persönlich wünsche \"Ich selbst habe nicht immer perfekt und erfolgreich zu sein.\". 0.53107253497887\n",
            "\n",
            "13\n",
            "\"Um ehrlich zu sein, mir ist das so was nicht wirklich wichtig.\" 9.788429383461836e-232\n",
            "\"Um ehrlich zu sein, es interessiert mich nicht wirklich um diese Dinge eher \"Ehrlich gesagt, sind schnuppe.\" 4.1804022597808986e-78\n",
            "\n",
            "14\n",
            "August in Deutschland veröffentlicht. 0\n",
            "bei uns ab er in Deutschland auf dem 25. August. Der Film läuft am 6.373848007830757e-78\n",
            "\n",
            "15\n",
            "Golfer Langer wird mit der Sportpyramide ausgezeichnet 7.711523862191631e-155\n",
            "Golfer Langer wird mit der Sportpyramide geehrt erhält die Sportpyramide 4.640083527732319e-78\n",
            "\n",
            "16\n",
            "Seine Erfahrung auf dem Pferd ist vernachlässigbar. 5.395774370246974e-78\n",
            "Seine Erfahrungen auf dem Pferdeweg sind vernachlässigbar. 0.4347208719449914\n",
            "\n",
            "17\n",
            "Es war das erste für den 58-Jährigen. 1.331960397810445e-231\n",
            "Für den 58-Jährigen war es eine Premiere. 1.0\n",
            "\n",
            "18\n",
            "CHIO: \"Goldene Sportpyramide\" für Bernhard Langer 1.0\n",
            "CHIO: \"Goldene Sportpyramide\" für Bernhard Langer 1.0\n",
            "\n",
            "19\n",
            "Nach einem Kilometer bei dieser Geschwindigkeit hatte ich Angst. 6.7393716283177006e-155\n",
            "Mit diesem Tempo, ich habe Angst gehabt. - Ein Kilometer beiahnten mich! 0.46924700641055994\n",
            "\n",
            "20\n",
            "\"Es war keine gute Erfahrung\", sagte Langer. 7.711523862191631e-155\n",
            "\"Es war kein gutes Erlebnis\", sagt Langer. - Was? 0.7259795291154771\n",
            "\n",
            "21\n",
            "Und damit endete seine Karriere im Reiten wieder. 4.2338646970116024e-78\n",
            "Und damit ging die Reitkarriere dann wieder zu Ende. Und das war auch schon wieder. 5.053216933905455e-78\n",
            "\n",
            "22\n",
            "Langers erstes Mal in Aachen und bei CHIO. 0.22172045047934608\n",
            "damit auch beim Langer war das erste Mal in Aachen. In der CHIO. 0.49735673561245436\n",
            "\n",
            "23\n",
            "Er war völlig überzeugt. 1.0032743411283238e-231\n",
            "Der Grund war ihm durchaus überzeugend. 7.262123179505913e-78\n",
            "\n",
            "24\n",
            "Der freundliche Sportler fehlt nicht für Auszeichnungen. 9.709385502639237e-232\n",
            "Der freundliche Athlet An Auszeichnungen mangelt es dem sympathischen Sportler nicht aus. 0.5156626918239822\n",
            "\n",
            "25\n",
            "Das Bundesverdienstkreuz, das Silber Laurel Blatt, Champions Tour Player of the Year... 5.791739854583281e-155\n",
            "Das Bundesverdienstkreuz, die Silber Silbernes Lorbeerblatt, Champions-Tour-Player des Jahres... 4.335118471269586e-78\n",
            "\n",
            "26\n",
            "Sogar die britische Königin hat ihm eine Ehre erwiesen. 6.7393716283177006e-155\n",
            "schon geadelt. Selbst die britische Queen hat ihn eine Ehre zuteil werden lassen! 0.42803206067505944\n",
            "\n",
            "27\n",
            "Langer ist die 18. Person, die mit der Sportpyramide ausgezeichnet wird. 6.513869329968086e-155\n",
            "Langer ist mit der Sportpyramide der 18. Preisträger der Sportpyramide. 0.4518010018049224\n",
            "\n",
            "28\n",
            "Am Samstag traf er den ersten Menschen, der ihn in Aachen empfing, Hans Günter Winkler. 4.13876789251507e-78\n",
            "In Aachen trafierte er den ersten Menschen am Samstag den Hans Gündner Winkl Preisträger Hans Günter. 4.1548410799625e-78\n",
            "\n",
            "29\n",
            "Langer fördert seit Jahren Nachwuchstalente. 1.1862177682648818e-231\n",
            "Seit Jahrzehnten fördert Langers Nachwuchstalent an. 5.775353993361614e-78\n",
            "\n",
            "30\n",
            "Und in dieser Hinsicht hat er sofort das Preisgeld von 25.000 Euro übergeben. 0.31455601883230705\n",
            "Und so gab er das Preisgeld in Höhe von 25.000 Euro sofort weiter. auch das Preisgeld. 0.2696698152099016\n",
            "\n",
            "31\n",
            "Viele dieser Sportlegenden kamen am Samstag nach Aachen. 6.626356351409518e-78\n",
            "Viele dieser Sportlegenden kamen samstags nach Aachen und waren am Samstag. 5.008605395783359e-78\n",
            "\n",
            "32\n",
            "Bernhard Langer hielt sich von den großen Tieren fern. 5.072841446586652e-78\n",
            "Bernhard Langer hielt Abstand zu den großen Tieren. 1.0\n",
            "\n",
            "33\n",
            "Vergleich der Wirtschaftspolitik von Clinton und Trump: Es geht um das Geld 1.1640469867513693e-231\n",
            "all about the Geld im Vergleich: It's ongeround money and Trump Clintons und Trumps Wirtschaftspolitik 0.31138788080750657\n",
            "\n",
            "34\n",
            "Sowohl Clinton als auch Trump wollen in Infrastruktur investieren und das TTP-Handelsabkommen verhindern. 0.229585358002991\n",
            "Handelsabkommen TTP verhindern sowohl Clinton als wie auch Trump auf Infrastruktur- das wollen. In Infrastruktur investieren. 5.072841446586652e-78\n",
            "\n",
            "35\n",
            "Das ist die Rolle des Staates. 0.5081327481546147\n",
            "Dies ist die Rolle des Staates Der, über die wir sprechen. 5.475733548646979e-78\n",
            "\n",
            "36\n",
            "Er würde die Dinge ganz anders machen, sagt Trump. 1.384292958842266e-231\n",
            "anders, behauptet Trump. selber dagegen wäre die Dinge ganz und gar nicht tun! 5.191033047549162e-78\n",
            "\n",
            "37\n",
            "Das würde die Steuern für die Reichen erhöhen, sagt sie. 0.4518010018049224\n",
            "Dies würde die Steuern für die Reichen erhöhen Deshalb würden die erhöht, sagt sie. 0.42718025135819776\n",
            "\n",
            "38\n",
            "Interessanterweise haben sie ähnliche Ansichten über die Infrastruktur. 1.1368587676511996e-231\n",
            "sehr nahe beim sind sich beidet haben. Thema Infrastruktur. 5.929498746342409e-78\n",
            "\n",
            "39\n",
            "Es muss Investitionen in Infrastruktur geben, sagen Trump und Clinton fast wörtlich. 0.31702331385234306\n",
            "Die In die müsse der Infrastruktur muss investiert werden, sagen Trump und Clinton fast wörtlich. 0.4920274515385508\n",
            "\n",
            "40\n",
            "Trump könnte daher zu Diskussionen innerhalb der Partei führen. 1.2508498911928379e-231\n",
            "Auf Trump könnten also noch parteiinterne Diskussionen innerhalb der Partei geführt werden. - Was? 0.4324227075463215\n",
            "\n",
            "41\n",
            "Es gibt klarere Unterschiede in der Energie- und Klimapolitik. 0.5873949094699213\n",
            "Bei der Energie- und Klimapolitik sind klarere Unterschiede zu erkennen. sichtigen Deutlicher sind die 4.887310777383362e-78\n",
            "\n",
            "42\n",
            "Auch nicht wollen TTP unterzeichnen, das Transpazifische Handelsabkommen. 1.4256605770826504e-231\n",
            "Handelsabkommen, wollen beide TTP auch nicht unterzeichnen. Das trans TPP, das transpazifische 6.0598953153840875e-78\n",
            "\n",
            "43\n",
            "Dann sollten sie auch besser bezahlt werden, fordert Clinton. 0.4180134288483488\n",
            "Die sollen dann sollten auch besser bezahlt werden, fordert Clinton. 0.5410822690539396\n",
            "\n",
            "44\n",
            "Der derzeitige Mindestlohn von 7,25 US-Dollar sei eine Schwäche, sagt sie. 0.5706745777055999\n",
            "Der derzeitige Mindestlohn von 7,25 US-Dollar sei ein Hungerlohn. 1.0\n",
            "\n",
            "45\n",
            "Sie will es auf 15 Dollar pro Stunde erhöhen. 0.6104735835807844\n",
            "Sie will es auf 15 Dollar pro Stunde erhöhen. Sie Ihn will sie. 0.42803206067505944\n",
            "\n",
            "46\n",
            "Seine Kritiker erklären, dass dies das Haushaltsdefizit nur erhöhen wird. 3.798979081005021e-78\n",
            "Seine Kritiker geben an, daß damit würde das halten ihm vor, sich zu erhöhen. 5.343126373842921e-78\n",
            "\n",
            "47\n",
            "Zuflucht suchen: \"Jede Flüchtlingin hat sich mit sexueller Gewalt beschäftigt\" 6.8489908526642754e-155\n",
            "Flucht: \"Jede geflüchtete Frau hat Erfahrungen über sexuelle Gewalt gemacht.\" 0.5169731539571706\n",
            "\n",
            "48\n",
            "Wenn sie in Deutschland ankommen, sind sie oft traumatisiert. 1.0\n",
            "Wenn sie in Deutschland ankommen, sind sie oft traumatisiert. 1.0\n",
            "\n",
            "49\n",
            "Ein Haus bietet Zuflucht. 1.0\n",
            "Ein Haus bietet Zuflucht. 1.0\n",
            "\n",
            "50\n",
            "Vier Frauen sitzen in einer großen Küche. 7.711523862191631e-155\n",
            "In einer großenem Küche sitzen vier die Frauen.. Frauen. 5.30941411456236e-78\n",
            "\n",
            "51\n",
            "Eine Frau hält ihren kleinen Sohn in den Armen. 0.4671379777282001\n",
            "Eine Frau hält ihren kleinen Sohn in den Armen. Sie hat ihren kleinen. 0.33260249505555045\n",
            "\n",
            "52\n",
            "Er schaut neugierig um den Raum, seine Augen weit. 1.4875195904069663e-231\n",
            "guckt mit großen in den Raum. Seine Augen sind weit, die Augen sind weit! 4.887310777383362e-78\n",
            "\n",
            "53\n",
            "Seine Mutter starrt auf die Tischplatte. 1.0\n",
            "Seine Mutter starrt auf die Tischplatte. 1.0\n",
            "\n",
            "54\n",
            "Bis sie merkt, dass sie beobachtet wird. 1.0\n",
            "Bis sie merkt, dass sie beobachtet wird. 1.0\n",
            "\n",
            "55\n",
            "Sie lächelt kurz und küsst das Kind. 0.6147881529512643\n",
            "Sie lächelt kurz und küsst das Kind auf den Kleinen. 0.4518010018049224\n",
            "\n",
            "56\n",
            "Die Szene sieht normal aus. 8.38826642100846e-155\n",
            "Die Szene wirkt normal. 1.0\n",
            "\n",
            "Beam size: 5, Boost: 15\n",
            "Baseline Avg BLEU: 0.26859017981201255, Constrained Avg BLEU: 0.3986776088198512, Sentences: 56\n",
            "\n",
            "Best Parameters:\n",
            "Beam size: 5, Constraint boost: 10\n",
            "Best Constrained BLEU: 0.4080912501115553\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-b4f9a405f5b1>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mconstraints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_constraints\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mde_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseline_translation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mnum_constraints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconstraint\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mconstraint\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconstraints\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconstraints\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mconstrained_translation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconstrained_beam_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstraints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0men_sentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_constraints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstraint_boost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m       \u001b[0mbaseline_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_BLEU_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbaseline_translation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mde_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-e2305fccb338>\u001b[0m in \u001b[0;36mconstrained_beam_search\u001b[0;34m(model, input_ids, constraints, max_len, num_constraints, constraint_boost, beam_size)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhyp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_open\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m           \u001b[0;31m# generate new hypotheses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m           \u001b[0mnew_hyps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstraint_boost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstraints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m           \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_hyps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-a995e3d109cd>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(model, hyp, input_ids, beam_size, encoder_outputs, constraint_boost, constraints)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     outputs = model(\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/marian/modeling_marian.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1399\u001b[0m                 )\n\u001b[1;32m   1400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1401\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m   1402\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1403\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/marian/modeling_marian.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m         decoder_outputs = self.decoder(\n\u001b[0m\u001b[1;32m   1196\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_input_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/marian/modeling_marian.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    993\u001b[0m                 )\n\u001b[1;32m    994\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 995\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    996\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    997\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/marian/modeling_marian.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    422\u001b[0m             \u001b[0;31m# cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0mcross_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m             hidden_states, cross_attn_weights, cross_attn_present_key_value = self.encoder_attn(\n\u001b[0m\u001b[1;32m    425\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m                 \u001b[0mkey_value_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/marian/modeling_marian.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_weights_reshaped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_BLEU_score_avgs(en_sentences, de_sentences):\n",
        "  baseline_score_avg = 0\n",
        "  constrained_score_avg = 0\n",
        "  sentence_count = 0\n",
        "\n",
        "  for en_sentence, de_sentence in zip(en_sentences[:100], de_sentences[:100]):\n",
        "    input_ids = tokenize(en_sentence)\n",
        "\n",
        "    if len(en_sentence.split()) > 15:\n",
        "      continue\n",
        "\n",
        "    baseline_translation = decode(generate_baseline_translation(model, input_ids).flatten().tolist())\n",
        "\n",
        "    if baseline_translation == de_sentence:\n",
        "      constrained_translation = baseline_translation\n",
        "    else:\n",
        "      constraints = extract_constraints(de_sentence, baseline_translation)\n",
        "      num_constraints = sum(len(constraint.split()) if \" \" in constraint else 1 for constraint in constraints) if len(constraints) > 0 else 0\n",
        "      constrained_translation = decode(constrained_beam_search(model, input_ids, constraints, len(en_sentence.split()) + 10, num_constraints).sequence.flatten().tolist())\n",
        "\n",
        "    baseline_score = compute_BLEU_score(baseline_translation, de_sentence)\n",
        "    constrained_score = compute_BLEU_score(constrained_translation, de_sentence)\n",
        "\n",
        "    sentence_count += 1\n",
        "\n",
        "    print(sentence_count)\n",
        "    print(baseline_translation, baseline_score)\n",
        "    print(constrained_translation, constrained_score)\n",
        "    print()\n",
        "\n",
        "    baseline_score_avg += baseline_score\n",
        "    constrained_score_avg += constrained_score\n",
        "\n",
        "  baseline_score_avg /= sentence_count\n",
        "  constrained_score_avg /= sentence_count\n",
        "\n",
        "  return baseline_score_avg, constrained_score_avg"
      ],
      "metadata": {
        "id": "X79sE8xxLd7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev_baseline_score_avg, dev_constrained_score_avg = calculate_BLEU_score_avgs(en_sentences_dev, de_sentences_dev)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bh9UeMzJFR2-",
        "outputId": "e6317572-f842-49eb-dd0a-051995ebd4ba"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "28-jähriger Chef fand tot in San Francisco Mall 0.38260294162784475\n",
            "Der 28-jährige Koch in San Francisco Mall wurde tot gefunden. 0.4111336169005197\n",
            "\n",
            "2\n",
            "Ein Sprecher von Sons & Daughters sagte, sie seien \"geschockt und verwüstet\" durch seinen Tod. 0.1772712285241271\n",
            "Tod \"schockiert und am Boden zerstört, dass sie über die Grenzen des Sons & Daughter- seien\". 0.3397898021621042\n",
            "\n",
            "3\n",
            "Unsere Gedanken und unser Beileid sind mit Franks Familie und Freunden in dieser schwierigen Zeit. 0.4480304273880272\n",
            "Unsere Gedanken und unser Beileid sind mit Franks Familie und Freunden in dieser schweren Zeit beibachtet. 0.537284965911771\n",
            "\n",
            "Warning: No finished hypotheses. Returning the best incomplete hypothesis.\n",
            "4\n",
            "\"Er hat eine Wohnung gefunden, er ist mit einem Mädchen zusammen\", sagte Louis Galicia zu KGO. 1.2882297539194154e-231\n",
            "\"Er hat eine Wohnung gefunden hatte eine eine Wohnung, fand 6.677571628641163e-155\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n",
            "Er war ein freundlicher Geist mit einem großen Herzen. 0.5969491792019646\n",
            "Er war ein gütiger Mensch mit einem großen Herzen. 0.5969491792019646\n",
            "\n",
            "6\n",
            "Er wollte nie in irgendeiner Art von Auseinandersetzung sein. 0.43167001068522526\n",
            "Er wollte nie in irgendeiner Art von Auseinandersetzung sein.. an irgendeiner Art. 0.33180774028439425\n",
            "\n",
            "7\n",
            "Er war der Bruder, der mit dem Fluss ging. 0.7259795291154771\n",
            "Er war der Bruder, der mit dem Strom schwamm. 1.0\n",
            "\n",
            "8\n",
            "Jeder, der Informationen hat, wird gebeten, die SFPD Tip Line unter 415-575-4444 anzurufen. 3.1640699269154553e-78\n",
            "Wer Informationen hat, wird gebeten das Hinweistelefon zu dem Fall an des SFPD unter Nummer 415-575-4444 anzurufen. 0.3972595463783053\n",
            "\n",
            "9\n",
            "Jennifer Aniston: Ich bin immer geplatzt 4.446808895758207e-78\n",
            "Jennifer Anis werde immer ins Schubladen gesteckt. 7.183445846156676e-155\n",
            "\n",
            "10\n",
            "Jennifer Aniston muss nicht immer perfekt oder erfolgreich sein. 1.0\n",
            "Jennifer Aniston muss nicht immer perfekt oder erfolgreich sein. 1.0\n",
            "\n",
            "11\n",
            "Das hat der Hollywood-Star in einem Interview deutlich gemacht. 0.6010525952194526\n",
            "Das hat der Hollywood-Star in einem Interview reichlich deutlich gemacht. . . jetzt unmissverständlich klar. 0.4546697236991713\n",
            "\n",
            "12\n",
            "\"Persönlich habe ich nicht den Wunsch, die ganze Zeit perfekt und erfolgreich zu sein.\" 0.3661926362999429\n",
            "\"Persönlich wünsche \"Ich selbst habe nicht immer perfekt und erfolgreich zu sein.\". 0.53107253497887\n",
            "\n",
            "13\n",
            "\"Um ehrlich zu sein, mir ist das so was nicht wirklich wichtig.\" 9.788429383461836e-232\n",
            "\"Um ehrlich zu sein, es interessiert mich nicht wirklich einen Scheiß diese Dinge eher zu \"Ehrlich gesagt, sind schnuppe.\" 3.826170841813515e-78\n",
            "\n",
            "14\n",
            "August in Deutschland veröffentlicht. 0\n",
            "bei uns ab er in Deutschland auf dem 25. August. Der Film läuft am 6.373848007830757e-78\n",
            "\n",
            "15\n",
            "Golfer Langer wird mit der Sportpyramide ausgezeichnet 7.711523862191631e-155\n",
            "Golfer Langer wird mit der Sportpyramide geehrt erhält die Sportpyramide 4.640083527732319e-78\n",
            "\n",
            "16\n",
            "Seine Erfahrung auf dem Pferd ist vernachlässigbar. 5.395774370246974e-78\n",
            "Seine Erfahrungen auf dem Pferdeweg sind vernachlässigbar. 0.4347208719449914\n",
            "\n",
            "17\n",
            "Es war das erste für den 58-Jährigen. 1.331960397810445e-231\n",
            "Für den 58-Jährigen war es eine Premiere. 1.0\n",
            "\n",
            "18\n",
            "CHIO: \"Goldene Sportpyramide\" für Bernhard Langer 1.0\n",
            "CHIO: \"Goldene Sportpyramide\" für Bernhard Langer 1.0\n",
            "\n",
            "19\n",
            "Nach einem Kilometer bei dieser Geschwindigkeit hatte ich Angst. 6.7393716283177006e-155\n",
            "Mit diesem Tempo, ich habe Angst gehabt. - Ein Kilometer bei 0.587728372510532\n",
            "\n",
            "20\n",
            "\"Es war keine gute Erfahrung\", sagte Langer. 7.711523862191631e-155\n",
            "kein gutes Erlebnis\", sagt Langer. 0.6703200460356393\n",
            "\n",
            "21\n",
            "Und damit endete seine Karriere im Reiten wieder. 4.2338646970116024e-78\n",
            "Und damit ging die Reitkarriere dann wieder zu Ende. Und das war auch schon wieder. 5.053216933905455e-78\n",
            "\n",
            "22\n",
            "Langers erstes Mal in Aachen und bei CHIO. 0.22172045047934608\n",
            "damit auch beim Langer war das erste Mal in Aachen. In der CHIO. 0.49735673561245436\n",
            "\n",
            "23\n",
            "Er war völlig überzeugt. 1.0032743411283238e-231\n",
            "Der Grund war ihm durchaus überzeugend. 7.262123179505913e-78\n",
            "\n",
            "24\n",
            "Der freundliche Sportler fehlt nicht für Auszeichnungen. 9.709385502639237e-232\n",
            "Der freundliche Athlet An Auszeichnungen mangelt es dem sympathischen Sportler nicht aus. 0.5156626918239822\n",
            "\n",
            "25\n",
            "Das Bundesverdienstkreuz, das Silber Laurel Blatt, Champions Tour Player of the Year... 5.791739854583281e-155\n",
            "Das Bundesverdienstkreuz, die Silber Silbernes Lorbeerblatt, Champions-Tour-Player des Jahres... 4.335118471269586e-78\n",
            "\n",
            "26\n",
            "Sogar die britische Königin hat ihm eine Ehre erwiesen. 6.7393716283177006e-155\n",
            "schon geadelt. Selbst die britische Queen hat ihn eine Ehre zuteil werden lassen! 0.42803206067505944\n",
            "\n",
            "27\n",
            "Langer ist die 18. Person, die mit der Sportpyramide ausgezeichnet wird. 6.513869329968086e-155\n",
            "Langer ist mit der Sportpyramide der 18. Preisträger der Sportpyramide. 0.4518010018049224\n",
            "\n",
            "28\n",
            "Am Samstag traf er den ersten Menschen, der ihn in Aachen empfing, Hans Günter Winkler. 4.13876789251507e-78\n",
            "In Aachen traf er am Samstag den ersten Empfänger, Hans Gündner Winkler. Preisträger Hans Günter 0.5828233954152654\n",
            "\n",
            "29\n",
            "Langer fördert seit Jahren Nachwuchstalente. 1.1862177682648818e-231\n",
            "Seit Jahrzehnten fördert Langers Nachwuchstalent an. 5.775353993361614e-78\n",
            "\n",
            "30\n",
            "Und in dieser Hinsicht hat er sofort das Preisgeld von 25.000 Euro übergeben. 0.31455601883230705\n",
            "Und so gab er das Preisgeld in Höhe von 25.000 Euro sofort weiter. auch das Preisgeld. 0.2696698152099016\n",
            "\n",
            "31\n",
            "Viele dieser Sportlegenden kamen am Samstag nach Aachen. 6.626356351409518e-78\n",
            "Viele dieser Sportlegenden kamen samstags nach Aachen und waren am Samstag. 5.008605395783359e-78\n",
            "\n",
            "32\n",
            "Bernhard Langer hielt sich von den großen Tieren fern. 5.072841446586652e-78\n",
            "Bernhard Langer hielt Abstand zu den großen Tieren. 1.0\n",
            "\n",
            "33\n",
            "Vergleich der Wirtschaftspolitik von Clinton und Trump: Es geht um das Geld 1.1640469867513693e-231\n",
            "all about the Geld im Vergleich: It's ongeround money and Trump Clintons und Trumps Wirtschaftspolitik 0.31138788080750657\n",
            "\n",
            "34\n",
            "Sowohl Clinton als auch Trump wollen in Infrastruktur investieren und das TTP-Handelsabkommen verhindern. 0.229585358002991\n",
            "sowohl Clinton alsen Trump In Infrastruktur investieren wollen und verhindern das Handelsabkommen TTP verhindern. - das wollen. 5.400726898975435e-78\n",
            "\n",
            "35\n",
            "Das ist die Rolle des Staates. 0.5081327481546147\n",
            "Dies ist die Rolle des Staates Der, über die wir sprechen. 5.475733548646979e-78\n",
            "\n",
            "36\n",
            "Er würde die Dinge ganz anders machen, sagt Trump. 1.384292958842266e-231\n",
            "anders, behauptet Trump. selber dagegen wäre die Dinge ganz und gar nicht tun! 5.191033047549162e-78\n",
            "\n",
            "37\n",
            "Das würde die Steuern für die Reichen erhöhen, sagt sie. 0.4518010018049224\n",
            "Dies würde die Steuern für die Reichen erhöhen Deshalb würden die erhöht, sagt sie. 0.42718025135819776\n",
            "\n",
            "38\n",
            "Interessanterweise haben sie ähnliche Ansichten über die Infrastruktur. 1.1368587676511996e-231\n",
            "sehr nahe beim sind sich beidet haben. Thema Infrastruktur. 5.929498746342409e-78\n",
            "\n",
            "39\n",
            "Es muss Investitionen in Infrastruktur geben, sagen Trump und Clinton fast wörtlich. 0.31702331385234306\n",
            "Die In die müsse der Infrastruktur muss investiert werden, sagen Trump und Clinton fast wörtlich. 0.4920274515385508\n",
            "\n",
            "40\n",
            "Trump könnte daher zu Diskussionen innerhalb der Partei führen. 1.2508498911928379e-231\n",
            "Auf Trump könnten also noch parteiinterne Diskussionen innerhalb der Partei geführt werden. - Was? 0.4324227075463215\n",
            "\n",
            "41\n",
            "Es gibt klarere Unterschiede in der Energie- und Klimapolitik. 0.5873949094699213\n",
            "Bei der Energie- und Klimapolitik sind klarere Unterschiede zu erkennen. sichtigen Deutlicher sind die 4.887310777383362e-78\n",
            "\n",
            "42\n",
            "Auch nicht wollen TTP unterzeichnen, das Transpazifische Handelsabkommen. 1.4256605770826504e-231\n",
            "Handelsabkommen, wollen beide TTP auch nicht unterzeichnen. Das trans TPP, das transpazifische 6.0598953153840875e-78\n",
            "\n",
            "43\n",
            "Dann sollten sie auch besser bezahlt werden, fordert Clinton. 0.4180134288483488\n",
            "Die sollen dann sollten auch besser bezahlt werden, fordert Clinton. 0.5410822690539396\n",
            "\n",
            "44\n",
            "Der derzeitige Mindestlohn von 7,25 US-Dollar sei eine Schwäche, sagt sie. 0.5706745777055999\n",
            "Der derzeitige Mindestlohn von 7,25 US-Dollar sei ein Hungerlohn. 1.0\n",
            "\n",
            "45\n",
            "Sie will es auf 15 Dollar pro Stunde erhöhen. 0.6104735835807844\n",
            "Sie will es auf 15 Dollar pro Stunde erhöhen. Sie Ihn will sie. 0.42803206067505944\n",
            "\n",
            "46\n",
            "Seine Kritiker erklären, dass dies das Haushaltsdefizit nur erhöhen wird. 3.798979081005021e-78\n",
            "Seine Kritiker geben an, dass die halten ihm vor, das Haushaltsdefizit zu erhöhen. damit würde das 5.171389238907095e-78\n",
            "\n",
            "47\n",
            "Zuflucht suchen: \"Jede Flüchtlingin hat sich mit sexueller Gewalt beschäftigt\" 6.8489908526642754e-155\n",
            "Refugium suchen: \"Jeder Flüchtling Frau hat Erfahrungen Flucht: \"Jede geflüchtete 6.206021746903507e-78\n",
            "\n",
            "48\n",
            "Wenn sie in Deutschland ankommen, sind sie oft traumatisiert. 1.0\n",
            "Wenn sie in Deutschland ankommen, sind sie oft traumatisiert. 1.0\n",
            "\n",
            "49\n",
            "Ein Haus bietet Zuflucht. 1.0\n",
            "Ein Haus bietet Zuflucht. 1.0\n",
            "\n",
            "50\n",
            "Vier Frauen sitzen in einer großen Küche. 7.711523862191631e-155\n",
            "In einer großenem Küche sitzen vier die Frauen. Frauen. 5.30941411456236e-78\n",
            "\n",
            "51\n",
            "Eine Frau hält ihren kleinen Sohn in den Armen. 0.4671379777282001\n",
            "Eine Frau hält ihren kleinen Sohn in den Armen. Sie hat ihren kleinen. 0.33260249505555045\n",
            "\n",
            "52\n",
            "Er schaut neugierig um den Raum, seine Augen weit. 1.4875195904069663e-231\n",
            "Er sieht in den Raum. Seine Augen sind guckt mit großenten Augen! 4.337579510280189e-78\n",
            "\n",
            "53\n",
            "Seine Mutter starrt auf die Tischplatte. 1.0\n",
            "Seine Mutter starrt auf die Tischplatte. 1.0\n",
            "\n",
            "54\n",
            "Bis sie merkt, dass sie beobachtet wird. 1.0\n",
            "Bis sie merkt, dass sie beobachtet wird. 1.0\n",
            "\n",
            "55\n",
            "Sie lächelt kurz und küsst das Kind. 0.6147881529512643\n",
            "Sie lächelt kurz und küsst das Kind auf den Kleinen. 0.4518010018049224\n",
            "\n",
            "56\n",
            "Die Szene sieht normal aus. 8.38826642100846e-155\n",
            "Die Szene wirkt normal. 1.0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dev_baseline_score_avg, dev_constrained_score_avg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6n38Q8SiWf9m",
        "outputId": "1af8638e-8e79-4fc6-ad3a-b23153e18cae"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.26859017981201255 0.4009753253283911\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_baseline_score_avg, test_constrained_score_avg = calculate_BLEU_score_avgs(en_sentences_test, de_sentences_test)"
      ],
      "metadata": {
        "id": "r5wWkRb6Fi9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_baseline_score_avg, test_constrained_score_avg)"
      ],
      "metadata": {
        "id": "i6fgfhDArBqc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPdF3SThyfw6vQaS63Hx3Oi"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f9c4f62fa256403983cc4f95cf82a3a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_df919452c981461ebb634224e546c675",
              "IPY_MODEL_dcd489fbc33b4163a6e54bf4384ccc43",
              "IPY_MODEL_181077a5596f4460b56cf93d81fda50a"
            ],
            "layout": "IPY_MODEL_60cf3cac9329420f93659b3291b6f3f8"
          }
        },
        "df919452c981461ebb634224e546c675": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9122e39f324446d9aea0908f017438e3",
            "placeholder": "​",
            "style": "IPY_MODEL_2a1829207ac3487b882fe4b94a973df2",
            "value": "Resolving data files: 100%"
          }
        },
        "dcd489fbc33b4163a6e54bf4384ccc43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b8fae2183cb44878f2baa7599801e00",
            "max": 17,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6c061e7036a7453cb9274db8059baa5f",
            "value": 17
          }
        },
        "181077a5596f4460b56cf93d81fda50a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_844a430e8f1a4e2fbc775f67ea1d10ab",
            "placeholder": "​",
            "style": "IPY_MODEL_140c3067e3554c32ae6b29d00392d175",
            "value": " 17/17 [00:00&lt;00:00, 372.28it/s]"
          }
        },
        "60cf3cac9329420f93659b3291b6f3f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9122e39f324446d9aea0908f017438e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a1829207ac3487b882fe4b94a973df2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4b8fae2183cb44878f2baa7599801e00": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c061e7036a7453cb9274db8059baa5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "844a430e8f1a4e2fbc775f67ea1d10ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "140c3067e3554c32ae6b29d00392d175": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2d527acf3231437a94541cd059afb91b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b314b67e1c2641a19a5d1d7deeb2ba5b",
              "IPY_MODEL_9a01ddff9be9495cba21abbe2321fa32",
              "IPY_MODEL_1a952edcc18c494183bc55de334d58f6"
            ],
            "layout": "IPY_MODEL_67b56fb927a04154a958d2bd29ad425a"
          }
        },
        "b314b67e1c2641a19a5d1d7deeb2ba5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3764b376b2ca4e25be1f12cf391e8694",
            "placeholder": "​",
            "style": "IPY_MODEL_0e8a01f09ce84f3093ee064c8f84c7f8",
            "value": "Resolving data files: 100%"
          }
        },
        "9a01ddff9be9495cba21abbe2321fa32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_976e1192b88241b2826b068ef1616895",
            "max": 17,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f9d42e468e3c4898975f7a79b0030e81",
            "value": 17
          }
        },
        "1a952edcc18c494183bc55de334d58f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2c625c6eee5443a9ef4ad953d5837f8",
            "placeholder": "​",
            "style": "IPY_MODEL_466b3a9e70f6407c96c0c4db9e249648",
            "value": " 17/17 [00:00&lt;00:00, 425.31it/s]"
          }
        },
        "67b56fb927a04154a958d2bd29ad425a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3764b376b2ca4e25be1f12cf391e8694": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e8a01f09ce84f3093ee064c8f84c7f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "976e1192b88241b2826b068ef1616895": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9d42e468e3c4898975f7a79b0030e81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d2c625c6eee5443a9ef4ad953d5837f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "466b3a9e70f6407c96c0c4db9e249648": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}