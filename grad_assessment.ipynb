{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ampham03/grad-assessment/blob/main/grad_assessment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "yZWgNlEHOdQ3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8bd0143-0f8e-4b71-a1ec-4bd3afab2903"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BcGN5ejymSEV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77d01c7a-5636-4339-d2b6-b05ccae88d60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "from datasets import load_dataset\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "11RDYzzXTXOw",
        "outputId": "17678945-3421-44cd-89d7-95ac64375d14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
          ]
        }
      ],
      "source": [
        "model_name = \"Helsinki-NLP/opus-mt-en-de\"  # English to German\n",
        "model = MarianMTModel.from_pretrained(model_name)\n",
        "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# load WMT dataset for de-en\n",
        "de_en_train = load_dataset(\"wmt19\", \"de-en\", split='train')\n",
        "de_en_dev = load_dataset(\"wmt19\", \"de-en\", split='validation')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "baerjzLfZvv3"
      },
      "outputs": [],
      "source": [
        "# get german sentences from the train dataset\n",
        "wmt_corpus = []\n",
        "for row in range(de_en_train.num_rows // 50):\n",
        "  wmt_corpus.append(de_en_train[row]['translation']['de'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "german_stopwords = stopwords.words('german')\n",
        "# extract 2-3 grams\n",
        "vectorizer = CountVectorizer(ngram_range=(2, 5), stop_words=german_stopwords)\n",
        "X = vectorizer.fit_transform(wmt_corpus)\n",
        "ngram_counts = np.array(X.sum(axis=0)).flatten()\n",
        "ngram_list = vectorizer.get_feature_names_out()\n",
        "total_ngrams = ngram_counts.sum()\n",
        "\n",
        "# extract unigrams\n",
        "unigram_vectorizer = CountVectorizer(ngram_range=(1, 1), stop_words=german_stopwords)\n",
        "X_uni = unigram_vectorizer.fit_transform(wmt_corpus)\n",
        "unigram_counts = np.array(X_uni.sum(axis=0)).flatten()\n",
        "unigram_list = unigram_vectorizer.get_feature_names_out()\n",
        "total_unigrams = unigram_counts.sum()\n"
      ],
      "metadata": {
        "id": "9OYKs-bUSK8r"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "0eH0P7xRuxlj"
      },
      "outputs": [],
      "source": [
        "ngram_probs = {\n",
        "  ngram: count / total_ngrams\n",
        "  for ngram, count in zip(ngram_list, ngram_counts)\n",
        "  if count >= 3\n",
        "}\n",
        "unigram_probs = {\n",
        "  unigram: count / total_unigrams\n",
        "  for unigram, count in zip(unigram_list, unigram_counts)\n",
        "  if count >= 3\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "0dnv3zCkbHm1"
      },
      "outputs": [],
      "source": [
        "def tokenize(sentence):\n",
        "  return tokenizer(sentence, return_tensors=\"pt\", padding=True).input_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "unTXIiDjvhTb"
      },
      "outputs": [],
      "source": [
        "class Hypothesis:\n",
        "  def __init__(self, score=0, is_open=True, sequence=None, constraints=[]):\n",
        "    self.score = score\n",
        "    self.is_open = is_open\n",
        "    self.sequence = sequence\n",
        "    self.constraints = constraints\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Jq1-dXz_wc4G"
      },
      "outputs": [],
      "source": [
        "def init_grid(max_len, num_constraints):\n",
        "  grid = []\n",
        "  for i in range(max_len):\n",
        "    row = []\n",
        "    for j in range(num_constraints + 1):\n",
        "        row.append([])\n",
        "    grid.append(row)\n",
        "  return grid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "t_QwUcPnueJV"
      },
      "outputs": [],
      "source": [
        "# generate new open hypotheses\n",
        "def generate(model, hyp, input_ids, constraints=[]):\n",
        "\n",
        "  if hyp.sequence is not None:\n",
        "    if hyp.sequence[-1, -1].item() == tokenizer.eos_token_id:\n",
        "      return None\n",
        "\n",
        "  # forward pass\n",
        "  with torch.no_grad():\n",
        "      encoder_outputs = model.get_encoder()(input_ids=input_ids)\n",
        "\n",
        "  # initialize decoder input\n",
        "  if hyp.sequence is None:\n",
        "      decoder_input_ids = torch.tensor([[model.config.decoder_start_token_id]])\n",
        "  else:\n",
        "      decoder_input_ids = hyp.sequence\n",
        "\n",
        "  with torch.no_grad():\n",
        "    outputs = model(\n",
        "        input_ids=None,\n",
        "        encoder_outputs=encoder_outputs,\n",
        "        decoder_input_ids=decoder_input_ids\n",
        "    )\n",
        "    logits = outputs.logits\n",
        "\n",
        "  # get the logits for the last predicted token\n",
        "  next_token_logits = logits[:, -1, :] # [batch_size, sequence_length, vocab_size]\n",
        "  next_token_probs = torch.softmax(next_token_logits, dim=-1).squeeze(0)\n",
        "\n",
        "  # get the token with the highest probability\n",
        "  next_token_id = torch.argmax(next_token_probs).item()\n",
        "  next_token_prob = next_token_probs[next_token_id].item()\n",
        "\n",
        "  new_score = hyp.score + next_token_prob\n",
        "  new_sequence = torch.cat([decoder_input_ids, torch.tensor([[next_token_id]])], dim=-1)\n",
        "\n",
        "  next_token = tokenizer.decode([next_token_id], skip_special_tokens=True)\n",
        "\n",
        "  # check if the decoded token is in the list of constraints\n",
        "  if next_token in constraints:\n",
        "    new_constraints = hyp.constraints + [next_token]\n",
        "  else:\n",
        "    new_constraints = hyp.constraints\n",
        "\n",
        "  new_hyp = Hypothesis(new_score, True, new_sequence, new_constraints)\n",
        "\n",
        "  return new_hyp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "JqatlMs70IRh"
      },
      "outputs": [],
      "source": [
        "# start new constrained hypotheses\n",
        "def start(model, hyp, input_ids, constraints):\n",
        "\n",
        "  if hyp.sequence is not None:\n",
        "    if hyp.sequence[-1, -1].item() == tokenizer.eos_token_id:\n",
        "      return None\n",
        "\n",
        "  for constraint in constraints:\n",
        "    if constraint not in hyp.constraints:\n",
        "\n",
        "      # get constraint tokens\n",
        "      constraint_token_ids = tokenizer.encode(constraint, add_special_tokens=False)\n",
        "\n",
        "      # forward pass\n",
        "      with torch.no_grad():\n",
        "          encoder_outputs = model.get_encoder()(input_ids=input_ids)\n",
        "\n",
        "      # initialize decoder input\n",
        "      if hyp.sequence is None:\n",
        "          decoder_input_ids = torch.tensor([[model.config.decoder_start_token_id]])\n",
        "      else:\n",
        "          decoder_input_ids = hyp.sequence\n",
        "\n",
        "      new_sequence = decoder_input_ids\n",
        "      new_score = hyp.score\n",
        "\n",
        "      for token_id in constraint_token_ids:\n",
        "        with torch.no_grad():\n",
        "          outputs = model(\n",
        "              input_ids=None,\n",
        "              encoder_outputs=encoder_outputs,\n",
        "              decoder_input_ids=decoder_input_ids\n",
        "          )\n",
        "          logits = outputs.logits\n",
        "\n",
        "        # get the logits for the last predicted token\n",
        "        next_token_logits = logits[:, -1, :] # [batch_size, sequence_length, vocab_size]\n",
        "        next_token_probs = torch.softmax(next_token_logits, dim=-1).squeeze(0)\n",
        "\n",
        "        # get probability of constraint token\n",
        "        constraint_token_prob = next_token_probs[token_id].item()\n",
        "\n",
        "        new_score = new_score + constraint_token_prob\n",
        "        new_sequence = torch.cat([new_sequence, torch.tensor([[token_id]])], dim=-1)\n",
        "\n",
        "      new_constraints = hyp.constraints + [constraint]\n",
        "\n",
        "      is_open = len(hyp.constraints) < len(constraints)\n",
        "\n",
        "      new_hyp = Hypothesis(new_score, is_open, new_sequence, new_constraints)\n",
        "      return new_hyp\n",
        "\n",
        "  return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "aBjf1B_f0KB6"
      },
      "outputs": [],
      "source": [
        "# continue unfinished hypotheses\n",
        "def cont(model, hyp, input_ids, constraints):\n",
        "  return generate(model, hyp, input_ids, constraints)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "fcXMutx4Q8v2"
      },
      "outputs": [],
      "source": [
        "def constrained_beam_search(model, input_ids, constraints, max_len, num_constraints, beam_size=3):\n",
        "\n",
        "  \"\"\"\n",
        "  model: model\n",
        "  input: example tokenized sentence\n",
        "  constraints: constraints\n",
        "  max_len: max_len of translation\n",
        "  num_constraints: # of constraints\n",
        "  beam_size: beam_size\n",
        "  \"\"\"\n",
        "  start_hyp = Hypothesis() # initial hypothesis\n",
        "  grid = init_grid(max_len, num_constraints) # initialize beams in grid\n",
        "  grid[0][0] = [start_hyp]\n",
        "\n",
        "  for t in range(1, max_len):\n",
        "    for c in range(max(0, (num_constraints + t) - max_len), min(t, num_constraints) + 1):\n",
        "      n, s, g = [], [], []\n",
        "\n",
        "      for hyp in grid[t - 1][c]:\n",
        "        if hyp.is_open:\n",
        "          new_hyp = generate(model, hyp, input_ids, constraints)\n",
        "          if new_hyp is not None:\n",
        "            g.append(new_hyp)\n",
        "\n",
        "      if c > 0:\n",
        "      for hyp in grid[t - 1][c - 1]:\n",
        "        if hyp.is_open:\n",
        "          new_hyp = start(model, hyp, input_ids, constraints)\n",
        "          if new_hyp is not None:\n",
        "            s.append(new_hyp)\n",
        "        else:\n",
        "          new_hyp = cont(model, hyp, input_ids, constraints)\n",
        "          if new_hyp is not None:\n",
        "            n.append(new_hyp)\n",
        "\n",
        "      all_hyps = sorted(n + s + g, key=lambda hyp: hyp.score, reverse=True)\n",
        "\n",
        "      grid[t][c] = all_hyps[:beam_size] # k-best scoring hypotheses stay on the beam\n",
        "\n",
        "  top_level_hyps = [] # get hyps in top level beams\n",
        "  for t in range(len(grid)):\n",
        "    top_level_hyps.extend(grid[t][num_constraints])\n",
        "\n",
        "  finished_hyps = []\n",
        "  for hyp in top_level_hyps:\n",
        "    if hyp.sequence[0, -1].item() == tokenizer.eos_token_id:\n",
        "      finished_hyps.append(hyp)\n",
        "\n",
        "  best_hyp = max(top_level_hyps, key=lambda hyp: hyp.score)\n",
        "\n",
        "  return best_hyp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJjL6nCcddZF",
        "outputId": "a5da3cf2-dc5e-472f-8372-c35de6391209"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3478224"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "de_en_train.num_rows // 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9BKkyWmoaisa"
      },
      "outputs": [],
      "source": [
        "def pmi(p_x_y, p_x_vals):\n",
        "  p_x = np.prod(p_x_vals)\n",
        "  return np.log(p_x_y / p_x) if p_x != 0 else 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2mMJo21hajIh"
      },
      "outputs": [],
      "source": [
        "def calculate_npmi(ngram, ngram_probs, unigram_probs):\n",
        "  ngram_prob = ngram_probs.get(ngram, 0)\n",
        "  if ngram_prob == 0:\n",
        "    return 0\n",
        "\n",
        "  words = ngram.split()\n",
        "\n",
        "  p_x_vals = [unigram_probs.get(word, 1e-9) for word in words]\n",
        "  pmi_value = pmi(ngram_prob, p_x_vals)\n",
        "\n",
        "  npmi_value = pmi_value / -np.log(ngram_prob)\n",
        "  return npmi_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "OedX2yvXUgZ3"
      },
      "outputs": [],
      "source": [
        "def extract_constraints(sentence, vectorizer, ngram_probs, unigram_probs, npmi_threshold=0.9):\n",
        "  try:\n",
        "    vectorizer.fit([sentence])\n",
        "    sentence_ngrams = vectorizer.get_feature_names_out()\n",
        "  except ValueError:\n",
        "    return []\n",
        "\n",
        "  constraints = []\n",
        "  for ngram in sentence_ngrams:\n",
        "    npmi = calculate_npmi(ngram, ngram_probs, unigram_probs)\n",
        "    if npmi != 0: # idk how to get better npmi\n",
        "      constraints.append(ngram)\n",
        "  return constraints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGLlhojkc5VQ",
        "outputId": "2c7e5b6f-d536-467a-e69d-39e70c3781e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ],
      "source": [
        "print(extract_constraints(de_en_dev[5]['translation']['de'], vectorizer, ngram_probs, unigram_probs))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(extract_constraints(de_en_dev[0]['translation']['de'], vectorizer, ngram_probs, unigram_probs))"
      ],
      "metadata": {
        "id": "eEwGch-3kg3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "de_en_dev[6]['translation']['de']\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Y4vbY9fFk4En",
        "outputId": "745d6319-bb70-4d03-8ad8-ef23a2e4b9eb"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Das kennt jeder, der sich schon mal aufregen musste, weil das Auto-Navi statt einer Umgehungsstraße eine grüne Wiese anzeigte.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7kF1ulMaeig",
        "outputId": "987a8876-2045-41f4-dd02-938438983eea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 []\n",
            "1 []\n",
            "2 []\n",
            "3 []\n",
            "4 []\n",
            "5 []\n",
            "6 ['grüne wiese', 'kennt schon', 'schon mal']\n",
            "7 ['gerade deshalb']\n",
            "8 []\n",
            "9 ['19 jahrhundert', 'seit 19', 'städte gemeinden']\n",
            "10 []\n",
            "11 ['städte gemeinden']\n",
            "12 []\n",
            "13 []\n",
            "14 ['seit frühjahr']\n",
            "15 ['land gab', 'militärischen zwecken', 'natürlich militärischen']\n",
            "16 ['französischen truppen']\n",
            "17 []\n",
            "18 ['stammt jahr']\n",
            "19 ['früher heute', 'heute beispiel', 'vergleich früher']\n",
            "20 []\n",
            "21 ['beispiel dafür']\n",
            "22 ['21 jahre', 'wurde 21']\n",
            "23 ['heute liegt']\n",
            "24 ['benannt wurde']\n",
            "25 ['ersten blick']\n",
            "26 ['heute landwirte', 'landwirte mehr']\n",
            "27 []\n",
            "28 ['19 jahrhundert']\n",
            "29 ['gibt historische']\n",
            "30 []\n",
            "31 []\n",
            "32 []\n",
            "33 ['zusammenhang tod']\n",
            "34 []\n",
            "35 []\n",
            "36 ['später wurde']\n",
            "37 ['mehr grund']\n",
            "38 ['befindet seit']\n",
            "39 []\n",
            "40 []\n",
            "41 ['1980er jahren']\n",
            "42 ['zerstört worden', 'zweiten weltkrieg']\n",
            "43 []\n",
            "44 ['zweite weltkrieg']\n",
            "45 []\n",
            "46 []\n",
            "47 []\n",
            "48 []\n",
            "49 []\n",
            "50 []\n",
            "51 ['28 august', 'besondere aufmerksamkeit', 'kraft treten', 'lassen reihe', 'letzter zeit', 'vorsicht walten', 'vorsicht walten lassen', 'walten lassen', 'zeit gesamten', 'äußerste vorsicht']\n",
            "52 []\n",
            "53 ['bürgerrechte menschen']\n",
            "54 ['sagte präsident']\n",
            "55 []\n",
            "56 []\n",
            "57 ['university of']\n",
            "58 ['anfang jahres', 'fragwürdigen umständen', 'genommen wurde', 'gewahrsam genommen']\n",
            "59 ['75 prozent', 'bericht büros', 'erschienenen bericht', 'kürzlich erschienenen', 'rat weist']\n",
            "60 ['darüber informieren', 'menschen bewusst']\n",
            "61 ['leute müssen', 'müssen bereit', 'wissen lassen']\n",
            "62 ['bezug anzahl', 'jahr 2015', 'neuesten zahlen']\n",
            "63 ['unternehmen wegen']\n",
            "64 ['american civil', 'american civil liberties', 'american civil liberties union', 'civil liberties', 'civil liberties union', 'liberties union', 'menschen wegen']\n",
            "65 ['innerhalb usa', 'letzter zeit']\n",
            "66 []\n",
            "67 ['kommenden jahr']\n",
            "68 ['ort bild']\n",
            "69 ['beiden abgeordneten']\n",
            "70 ['ebenso viele', 'hektar wald']\n",
            "71 []\n",
            "72 []\n",
            "73 []\n",
            "74 []\n",
            "75 []\n",
            "76 []\n",
            "77 []\n",
            "78 []\n",
            "79 ['euro pro', 'euro pro hektar', 'pro hektar']\n",
            "80 ['ganze weile']\n",
            "81 []\n",
            "82 ['landwirte stehen', 'oft kritik', 'stehen wegen']\n",
            "83 ['insgesamt 25', 'jahr 2013', 'wurden jahr']\n",
            "84 ['beträgt 40']\n",
            "85 ['500 euro', 'euro wurde']\n",
            "86 []\n",
            "87 ['große vorteil']\n",
            "88 ['macht richtig']\n",
            "89 ['stand fest']\n",
            "90 ['baden württemberg', 'staatliche förderung']\n",
            "91 []\n",
            "92 []\n",
            "93 ['außerdem darf']\n",
            "94 ['läuft derzeit']\n",
            "95 ['000 euro', 'liegen mehr']\n",
            "96 []\n",
            "97 ['weiße haus']\n",
            "98 []\n",
            "99 ['jahr 2003', 'saddam husseins', 'sturz saddam', 'sturz saddam husseins']\n"
          ]
        }
      ],
      "source": [
        "for row in range(100):\n",
        "  print(row, extract_constraints(de_en_dev[row]['translation']['de'], vectorizer, ngram_probs, unigram_probs))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_baseline_translation(model, input_ids):\n",
        "  hyp = Hypothesis()\n",
        "  while True:\n",
        "    hyp = generate(model, hyp, input_ids)\n",
        "\n",
        "    if hyp.sequence[0, -1].item() == tokenizer.eos_token_id:\n",
        "      break\n",
        "\n",
        "  return hyp.sequence"
      ],
      "metadata": {
        "id": "9DrDERSJu_mQ"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tokenize(\"He was also an anti- smoking activist and took part in several campaigns.\")\n",
        "constraints = extract_constraints(de_en_dev[11]['translation']['de'], vectorizer, ngram_probs, unigram_probs)\n",
        "constraints = ['Ebenso setzte er', 'gegen das Rauchen', 'nahm']"
      ],
      "metadata": {
        "id": "kP-XJVjavyoE"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "FF_6F4eifJPg"
      },
      "outputs": [],
      "source": [
        "constrained_translation = constrained_beam_search(model, input_ids, constraints, 30, len(constraints))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_translation = generate_baseline_translation(model, input_ids)"
      ],
      "metadata": {
        "id": "MS_eehULvspV"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_translation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEp5NkI7yTmN",
        "outputId": "7f580009-5cbf-4471-fcb9-5d578f603c3f"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[58100,   201,   133,    89,  2226,    13,   623,  9515,    13, 32533,\n",
              "           651,    10,  3549,    39,  4356, 30902,  6143,     3,     0]])"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "constrained_translation.sequence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMeznRf4uIX9",
        "outputId": "89c2138d-8347-4cd9-ab1c-50e71fe941bd"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[58100,   201,   133,    89,  2226,    13,   623,  9515,    13, 32533,\n",
              "           651,    10,   155,  1487,  1769,   452,   239,   227,   110,   372,\n",
              "           749,    44, 20437,  1017,     3,  4638,  5616,   227,    39,  4356,\n",
              "         30902,  6143,     3,     0]])"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "constrained_translation.score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7zRd_3ax5am",
        "outputId": "7ed2587c-022b-421f-ded2-e544a2311963"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11.76569245658041"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#baseline\n",
        "tokenizer.decode(torch.tensor([[58100,   201,   133,    89,  2226,    13,   623,  9515,    13, 32533,\n",
        "           651,    10,  3549,    39,  4356, 30902,  6143,     3,     0]]).flatten(), skip_special_tokens= True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "E5plbbFZyc4F",
        "outputId": "8b577714-3ddf-4a29-85d4-6d9c5346132b"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Er war auch Anti-Rauch-Aktivist und nahm an mehreren Kampagnen teil.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "dB1xgqTPrMX-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ad734d39-8cbc-40bf-acd5-c27a33fcdb9c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Er war auch Anti-Rauch-Aktivist und Ebenso setzte er gegen das Rauchen. nahmte an mehreren Kampagnen teil.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 110
        }
      ],
      "source": [
        "#constrained\n",
        "tokenizer.decode(torch.tensor([[558100,   201,   133,    89,  2226,    13,   623,  9515,    13, 32533,\n",
        "           651,    10,   155,  1487,  1769,   452,   239,   227,   110,   372,\n",
        "           749,    44, 20437,  1017,     3,  4638,  5616,   227,    39,  4356,\n",
        "         30902,  6143,     3,     0]]).flatten(), skip_special_tokens= True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "constraints"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNKOfNEYul55",
        "outputId": "92f83609-1c9f-4aa7-bc9f-bb5192b3efec"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Ebenso setzte er', 'gegen das Rauchen', 'nahm']"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reference\n",
        "de_en_dev[11]['translation']['de']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "dCOt5Ih9uWa-",
        "outputId": "8d8b8b7b-f200-4434-a036-d07fd24c07a4"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'In die große Übersichtskarte wurden für Städte und Gemeinden detailliertere Karten, sogenannte Urpositionsblätter, eingearbeitet.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "de_en_dev[11]['translation']['en']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "rMdVUwWmuZZ8",
        "outputId": "05365304-94bf-4e8f-f9f3-f28ffe53eb75"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Within the large overview map, were worked in detailed maps for towns and municipalities, so-called original-lay-of-the-land sheets.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5E16b_upjqIi"
      },
      "outputs": [],
      "source": [
        "# compute score using BLEU\n",
        "def compute_score(prediction, actual):\n",
        "  pass"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOypHwo3rKN05qI+1+oylWw",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}